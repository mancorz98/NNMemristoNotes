\documentclass[a4,journal]{IEEEtran}
% ensure proper font encoding and provide Times-compatible fonts (fixes TU/ptm/b/n undefined)
\usepackage[T1]{fontenc}
\usepackage{newtxtext,newtxmath}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\ifCLASSOPTIONcompsoc
    \usepackage[caption=false, font=normalsize, labelfont=sf, textfont=sf]{subfig}
\else
\usepackage[caption=false, font=footnotesize]{subfig}
\fi
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{balance}

%\usepackage[extract=python]{memoize} %ZG
% -- ADDED PACKAGES  after memoize --
\usepackage{graphicx}
\usepackage{circuitikz}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{makecell}


% optional packages - NOTES
\usepackage{ifthen}


%Added packages by KB in 4 June 2024
\usepackage{etoolbox}
\usepackage{listofitems}





\usetikzlibrary{shapes.geometric, arrows, positioning, fit, backgrounds, decorations.pathreplacing,shapes.geometric,arrows.meta,positioning,fit,backgrounds, calc}



\definecolor{ieeeblue}{RGB}{0,118,186}      % IEEE Blue
\definecolor{ieeered}{RGB}{214,39,40}       % IEEE Red  
\definecolor{ieeegreen}{RGB}{23,156,82}     % IEEE Green
\definecolor{red}{RGB}{255,0,0}             % Red
\definecolor{blue}{RGB}{0,0,255}            % Blue
\definecolor{darkgreen}{RGB}{0,180,0}  % Dark Green


\newcounter{todocounter}

\makeatletter
\newcommand{\listoftodos}{\section*{List of TODOs}\@starttoc{tod}}
\newcommand{\l@todo}[2]{#1 \dotfill #2\par}
\makeatother


\newcommand{\TODO}[2][]{%
  {\color{red} TODO: #2}%
  \ifthenelse{\equal{#1}{}}{}{%
    {\color{cyan}---#1}%
  }%
}

\newcommand\changed[2][pending]{%
  \ifthenelse{\equal{#1}{approved}}{#2}{{\color{blue}#2}}%
}
\renewcommand\comment[2][pending]{%
  \ifthenelse{\equal{#1}{approved}}{#2}{{\color{green}#2}}%
}

\newcommand{\der}{{\rm d}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\G}{\mathcal{G}} %conductance
\newcommand{\Gm}{\mathcal{G}} %conductance
\newcommand{\snum}{N} %sample number
\newcommand{\sind}{n} %sample index
\newcommand{\Ron}{R_{\rm ON}}
\newcommand{\Roff}{R_{\rm OFF}}
\newcommand{\von}{V_{\rm ON}}
\newcommand{\voff}{V_{\rm OFF}}
\newcommand{\q}{q}
\newcommand{\ua}{v}
\newcommand{\ia}{i}
\newcommand{\phia}{\varphi}
\newcommand{\xw}{x}
\newcommand{\dert}[1]{\frac{{\rm d} {#1}}{{\rm d} t} }
\newcommand{\inv}[1]{\frac{1}{#1} }
\newcommand{\equal}{=}
%ZG replaced \(V_s\) by \(V_{\mathrm s}\)
\newcommand{\Vs}{V_{\rm s}}
\newcommand{\vs}{v_{\rm s}}
\newcommand{\vr}{v_{\rm r}}
%ZG2 replaced "v_{\mathrm m}" by "v", replaced "i_{\mathrm m}" by "i"
\newcommand{\vm}{v}  % memristor voltage
\newcommand{\im}{i}  % memristor current
\newcommand{\imd}{i_{\rm md}} % memory dependent current
\newcommand{\Rs}{R_{\rm s}}
\newcommand{\Hz}{\,{\rm Hz}} %Herce powinny być raczej w normalnym foncie roman a nie matematycznym, ``z'' wygląda dziwnie
\newcommand{\V}{\,{\rm V}} %podobnie wolty
\newcommand{\Ohm}{\,\Omega} %podobnie wolty
\newcommand{\kOhm}{\,{\rm k}\Omega}
\newcommand{\mS}{\,{\rm mS}}
\newcommand{\muS}{\,{\rm \mu S}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\LossBase}{\Loss_{\rm b}}
\newcommand{\LossVel}{\Loss_{\rm v}}
\newcommand{\LossCur}{\Loss_{\rm c}}

%ZG2 added next lines
\newcommand{\MemODEOne}{SingleNN-MemODE}
\newcommand{\MemODETwo}{DualNN-MemODE}
\newcommand{\MSE}{\mathrm{MSE}}
\newcommand{\NMSE}{\mathrm{NMSE}}


\newcommand{\redbar}{\protect\tikz[baseline=-0.1ex]\fill[ieeered] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\bluebar}{\protect\tikz[baseline=-0.1ex]\fill[ieeeblue] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\greenbar}{\protect\tikz[baseline=-0.1ex]\fill[ieeegreen] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\mtimes}{\!\times\!}

%ZG commented out
%\usepackage[ 
%  backend=biber,
%  style=ieee,
%  minnames=1,
%  maxcitenames=2, maxbibnames=6
%]{biblatex}

%\addbibresource{bibliography.bib} %ZG


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\begin{document}
\title{Neural ODE based modeling of memristive devices}
\author{Karol Bednarz, Bartłomiej Garda, Zbigniew Galias,~\IEEEmembership{Senior Member,~IEEE} %ZG
  \thanks{Manuscript received Month ??, 2026. This work was supported by the National Science Centre, Poland under Grant 2024/53/B/ST7/03841.}
  \thanks{The authors are with the Department of Electrical Engineering, AGH University of Krak\'ow, al. Mickiewicza 30, 30--059, Krak\'ow, Poland, (e-mail: kbednarz@agh.edu.pl).} %ZG
}

\markboth{IEEE Transactions on Circuits and Systems,~Vol.~??, No.~?, Month~Year} %ZG
{Neural ODE based modeling of memristive circuits} %ZG

\maketitle

\begin{abstract}
  The development of modern memristive technologies requires accurate dynamic models to enable reliable simulation and prototyping in applications such as resistive RAM, neural networks, and neuromorphic computing. Existing symbolic models often exhibit limitations in accurately capturing the complex dynamic behavior of physical devices across various operating conditions, impacting their reliability in simulations. This work proposes a neural-network-based modeling of real memristors behaviour using Neural Ordinary Differential Equation (Neural ODE) models. Prior neural-network-based studies were limited by training data derived solely from theoretical models, preventing reliable assessment on real devices. The proposed method leverages a backpropagation technique based on solving the corresponding adjoint ODE backward in time using adjoint sensitivity methods. Experimental data obtained directly from Self-Directed Channel (SDC) memristors are used to train and validate the model. Results show that the Neural ODE approach more accurately reproduces the device’s dynamic characteristics and provides improved flexibility and robustness compared to traditional theoretical models. These findings demonstrate the feasibility of Neural ODE–based frameworks for data-driven modeling of a broad class of physical memristive devices. %ZG
\end{abstract}

\begin{IEEEkeywords}
  Memristor modeling, artificial neural network, neural ODE. %ZG
\end{IEEEkeywords} 

% \listoftodos %KB
\section{Introduction}
\IEEEPARstart{N}{umerous} memristor models have been proposed in the scientific literature following the discovery of memristive behavior at the nanoscale. The original model, introduced in~\cite{Strukov2008}, conceptualizes the memristor as a series connection of two variable resistances corresponding to the conductive and insulating regions of the thin film.
The generalized \textit{Mean Metastable Switch} (MMS) model was proposed in~\cite{Molter2016} for a more accurate modeling of Self-Directed Channel (SDC) memristors. In this approach based on semi-empirical formulation, the time derivative of the internal state variable is defined as a function of both the transition probabilities between metastable states and the current value of the state variable~\cite{Molter2016}. While existing symbolic models can accurately simulate memristor behavior under narrowly defined input conditions (e.g., specific signal shape, amplitude, or frequency), their generalization capability and reliability significantly diminish when the parameters of the driving signal are altered. This limitation is particularly challenging for applications like modeling memristors in chaotic oscillators, where current and voltage of the device undergoes continuous and highly dynamic changes. In~\cite{Lee2024}, the authors attempted to model memristive behavior using the \textit{Physics-Informed Neural Networks} (PINNs) framework. Although the reported results indicate the potential of this method, the study is not grounded in experimental measurements of physical memristor devices. Instead, the analysis is limited to comparisons with the outcomes of existing simulation models. Moreover, the training data employed during the neural network learning process were generated based on previously developed theoretical models, thereby limiting the ability to assess the method's accuracy in the context of real-world physical systems.


In~\cite{Ricky2018}, the authors introduce the concept of deep neural models in which the dynamics of the hidden state of a dynamical system is governed by an \textit{ordinary differential equation} (ODE). The training process is performed in an end-to-end manner, meaning that all parameters are optimized simultaneously within a single training procedure. A key innovation of the proposed approach is a novel backpropagation technique, which relies on solving the corresponding adjoint ODE backward in time using \textit{adjoint sensitivity methods}. This formulation enables efficient gradient computation and facilitates the application of neural ODEs in various architectures, including continuous-depth residual networks and generative flow-based models.


In~\cite{Ricky2021}, the authors extend the classical Neural ODE framework to enable the modeling of discontinuous events in continuous time—without requiring prior knowledge of the number or timing of such events. The proposed differentiable event functions allow for efficient simulation of hybrid systems, such as systems with collisions, state-switching mechanisms, or point processes. This development opens up new possibilities for modeling and training systems with discrete control inputs and non-smooth dynamics.


The present study aims to investigate the feasibility of modeling memristor devices using artificial neural networks, and to compare the effectiveness of this approach with that of existing theoretical models. Specifically, we focus on the application of Neural Ordinary Differential Equation (Neural ODE) models to simulate the behavior of SDC memristors, utilizing experimental data obtained from real physical systems.

Our objective is to assess whether neural network-based models can accurately reproduce the dynamics of SDC memristors, and to determine whether they offer any advantages over traditional theoretical approaches—particularly in terms of modeling flexibility, accuracy, and applicability to real-world, measurement-driven scenarios.

The remaining part of this paper is organized as follows. In Sec.~\ref{sec:NNModeling}, the neural network architecture is presented and two neural network-based models (SingleNN-MemODE and DualNN-MemODE) are proposed. The objective function defining the optimization problem to be solved is defined and existing memristors models used for comparison are briefly described. In Sec.~\ref{sec:results}, proposed models are trained using experimental data to obtain accurate models of real SDC memristors. The results are compared with the results obtained using existing methods. The influence of hyperparameters of the proposed methods on their performance is discussed. The last section concludes the study.


\section{Modeling Memristors using Neural Networks}
\label{sec:NNModeling}

\noindent This study utilizes experimental data obtained from Self-Directed Channel (SDC) memristors doped with tungsten. The structure and properties of these devices have been described in detail in the literature, e.g., in~\cite{Campbell2017,Garda2024}.

The experimental setup, illustrated in Fig.~\ref{fig:memristor_setup}, consists of an SDC memristor connected in series with a resistor. The sinusoidal input voltage is supplied by an arbitrary waveform generator. Voltage signals are measured using a data acquisition (DAQ) system.
%
Measurements are carried out for the following combinations of supply voltage amplitudes and frequencies: \(\Vs  \in \{0.5,\ 1.0,\ 1.5\}~[{\rm V}]\), \(f \in \{1,\ 5,\ 10,\ 20,\ 50,\ 100\}~[\rm Hz]\).


\begin{figure}[!t]
  \centering
  \resizebox{2.5in}{!}{\input{graphs/schematic.tex}}
  \vspace{-0.5in}
  \caption{Schematic diagram of the measurement setup used for the SDC memristor. The device is connected in series with a resistor, and the input voltage is applied via an arbitrary waveform generator. Voltage measurements are acquired using a data acquisition (DAQ) system.}
  \label{fig:memristor_setup}
\end{figure}

The analyzed model of a memristor follows the general theoretical framework of memristive devices originally introduced in~\cite{Chua1976}, and is expressed as
\newcommand{\xvec}{\mathbf{x}}
\begin{subequations}
  \label{eq:memristor_model}
  \begin{align}
    \ia(t)                    & = \G \left(\xvec(t),\ua(t)\right) \ua(t), \label{eq:memristor_model1} \\
    \frac{\der \xvec}{\der t} & = f\left(\xvec(t),\ua(t)\right), \label{eq:memristor_model2}
  \end{align}
\end{subequations}
where \(\xvec\) denotes the vector of internal state variables, whose temporal evolution is governed by the function \(f\), and \(\G\) represents the memductance of the device. The fundamental challenge in memristor modeling lies in accurately identifying the functional forms of \(\G(\xvec,\ua)\) and \(f(\xvec,\ua)\).


\subsection{Objective Function}
\noindent The objective function \(\Loss\) to be minimized in the optimization process is based on trajectories generated by the dynamical system. It is designed to simultaneously account for both the signal values and its dynamic structure. To this end, it is defined as the sum of four components, each serving a distinct and complementary role in the optimization process
\begin{align}
  \Loss & = \lambda_{\rm base} \LossBase + \lambda_{\rm vel} \LossVel + \lambda_{\rm curv} \LossCur + \sum^{N_c}_{i=1} \lambda_{{\rm con}_i} \Loss_{{\rm con}_i}(x).\label{eq:loss_function}
\end{align}
The first term \(\LossBase\) denotes the primary error computed as the mean squared error (MSE) between predicted and reference values. \(\LossVel\) enforces accurate modeling of derivatives of state variables, \(\LossCur\) promotes consistency of the trajectories in terms of their curvature by computing MSE of second derivatives, and \(\Loss_{\rm con}(x)\) enforces fulfillment of physical or structural constraints that the model is required to satisfy, including conformity to the underlying physical model.
The weighting coefficients \(\lambda_{\mathrm{base}}\), \(\lambda_{\mathrm{vel}}\), and \(\lambda_{\mathrm{curv}}\) balance the influence of each term according to the priorities of the modeling task.
Their values are calculated during each optimization step using the Geometric Loss Strategy (GLS)~\cite{Cipolla2018, Chen2018} using the Algorithm~\ref{alg:adaptive_loss_balancing}.

\begin{algorithm}[!t]
  \caption{Adaptive Loss Balancing with Clamping}
  \label{alg:adaptive_loss_balancing}
  \begin{algorithmic}[1]
    \REQUIRE Loss terms \(\LossBase, \LossVel, \LossCur\), \(\varepsilon > 0\), bounds \([\lambda_{\min}, \lambda_{\max}]\)
    \ENSURE Adaptive weights \(\lambda_{\text{base}}, \lambda_{\text{vel}}, \lambda_{\text{curv}}\)
    \STATE Compute the geometric mean: \\
    \(\overline{\Loss} \gets \exp\left(\frac{1}{3} \left(\ln(\LossBase\!+\!\varepsilon)+\ln(\LossVel\!+\!\varepsilon)+\ln(\LossCur\!+\!\varepsilon)\right)\right)\)
    \STATE \(\lambda_{\text{base}} \gets \text{clamp}(\frac{\overline{\Loss}}{\LossBase + \varepsilon}, \lambda_{\min}, \lambda_{\max})\)
    \STATE \(\lambda_{\text{vel}} \gets \text{clamp}(\frac{\overline{\Loss}}{\LossVel + \varepsilon}, \lambda_{\min}, \lambda_{\max})\)
    \STATE \(\lambda_{\text{curv}} \gets \text{clamp}(\frac{\overline{\Loss}}{\LossCur + \varepsilon}, \lambda_{\min}, \lambda_{\max})\)
    \STATE \textbf{return} \((\lambda_{\text{base}}, \lambda_{\text{vel}}, \lambda_{\text{curv}})\)
  \end{algorithmic}
\end{algorithm}

Let us consider two sequences \(z_k=(z_{k,\sind})_{\sind=1}^{\snum}\) and \(\tilde{z}_k=(\tilde{z}_{k,\sind})_{\snum=1}^{\snum}\), where \(z \in \{v,i,\dot{v},\dot{i},\ddot{v},\ddot{i}\}\) is a selected variable (voltage or current) or its derivative obtained from measurements, \(\tilde{z}\) is the predicted value of \(z\), \(\snum\) is the number of samples and \(k = 1,2,\ldots, N_{\text{traj}}\) is the sequence index.
The sequences are normalized utilizing the standard deviation to ensure statistical consistency within each dataset.
The trajectory-specific standard deviation is calculated using the unbiased estimator
\begin{equation}
  \sigma_{z,k} = \sqrt{\frac{1}{\snum-1} \sum_{\sind=1}^{\snum} \left(z_{k,\sind} - \bar{z}_k\right)^2},
\end{equation}
where the trajectory temporal mean is defined as \(\bar{z}_k = \frac{1}{\snum} \sum_{\sind=1}^{\snum} z_{k,\sind}\).

By implementing trajectory-specific normalization rather than global standardization, the methodology preserves the inherent temporal dynamics and statistical properties unique to each circuit configuration while simultaneously ensuring that amplitude disparities do not introduce systematic bias during predictive modeling procedures.
This approach is particularly advantageous in scenarios involving varying operational conditions, where maintaining the relative temporal structure is paramount for accurate system identification and dynamic analysis.
The normalized mean squared error between sequences \(({z}_{k,\sind})_{\sind=1}^{\snum}\) and \((\tilde{z}_{k,\sind})_{\sind=1}^{\snum}\) is defined as
\begin{equation}
%  \NMSE(z_k,\tilde{z}_k)=\MSE\left(\frac{z_k}{\sigma_{z,k}},\frac{\tilde{z}_k}{\sigma_{z,k}}\right)=\frac{1}{\snum}\sum_{\sind=1}^{\snum}\frac{({z}_{k,\sind}\!-\!\tilde{z}_{k,\sind})^2}{\sigma_{z,k}^2}.\\
  \NMSE(z_k,\tilde{z}_k)=\MSE\left(\frac{z_k}{\sigma_{z,k}},\frac{\tilde{z}_k}{\sigma_{z,k}}\right)=\frac{1}{\snum}\sum_{\sind=1}^{\snum}\left(\frac{{z}_{k,\sind}}{\sigma_{z,k}} -\frac{\tilde{z}_{k,\sind}}{\sigma_{z,k}}\right)^2.
\end{equation}


The primary component of the loss function is responsible for model fitting to experimental voltage-current data:

\begin{equation}
  \LossBase = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}}\left(\NMSE(v_k,\tilde{v}_k)+\NMSE(i_k,\tilde{i}_k)\right),\\
  \label{eq:base_loss}
\end{equation}
where \(\tilde{v}_{k}\) and \(\tilde{i}_{k}\) represent the predicted voltage and current for the \(k\)th trajectory, while \(v_{k}\) and \(i_{k}\) denote the corresponding experimental measurements.


The first-order dynamics comparison \(\LossVel\) enables evaluation of temporal derivative agreement between model predictions and experimental data, particularly critical for capturing rapid memristor state transitions
\begin{equation}
  \LossVel = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}}\left(\NMSE(\dot{v}_k,\tilde{\dot{v}}_k)+\NMSE(\dot{i}_k,\tilde{\dot{i}}_k)\right). 
  \label{eq:loss_v}
\end{equation}

The second-order comparison term \(\LossCur\) addresses trajectory curvature characteristics
\begin{equation}
  \LossCur = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}}\left(\NMSE(\ddot{v}_k,\tilde{\ddot{v}}_k)+\NMSE(\ddot{i}_k,\tilde{\ddot{i}}_k)\right). 
  \label{eq:loss_c}
\end{equation}
This component facilitates superior representation of curvature properties within the phase-space trajectory, particularly important for modeling systems exhibiting complex nonlinear and highly dynamic responses.


To enforce parameter bounds within specified intervals, a sophisticated constraint loss function utilizing smooth sigmoid transitions is implemented. This component prevents parameter drift beyond physically meaningful ranges while maintaining differentiability for gradient-based optimization algorithms. The constraint loss for the admissibility interval \(x\in[a,b]\) is expressed as:
%ZG \sigma used in the next equation, merged two lines into one
\begin{equation}
  \Loss_{\rm con}(x) = \mathbb{E}\left[ {\sigma\left(\tfrac{a-x}{\gamma}\right)}(a-x)^{2}+{\sigma\left(\tfrac{x-b}{\gamma}\right)}(x-b)^{2}\right],
  \label{eq:loss_con}
\end{equation}
where \(\sigma(z) = (1+e^{-z})^{-1} \) is the sigmoid function, \(a\) and \(b\) denote the lower and upper bounds of the admissible parameter range,
\(\gamma\) is a scaling factor controlling the steepness of the transition near the bounds, and
\(\mathbb{E}[\cdot]\) denotes the expected value (ensemble average over samples or trajectories).
The term \(\sigma((a-x)/\gamma)(a-x)^2\) penalizes values of \(x\) below the lower bound \(a\),
while the term \(\sigma((x-b)/\gamma)(x-b)^2\) penalizes values of \(x\) above the upper bound \(b\).
Squaring ensures that the penalty is non-negative and grows quadratically with the violation magnitude.

The operational characteristics of the constraint function \(\Loss_{\rm con}(x)\) are illustrated in Fig.~\ref{fig:constrains} for the parameter configuration: \(a=0\), \(b=1\), \(\gamma=0.0025\) over the domain \(x \in (-0.5, 1.5)\).
The function \(\Loss_{\rm con}(x)\) exhibits minimal penalty in the region \(x\in(0,1)\) and progressively increasing penalties as the parameter \(x\) exceeds the boundary limits.

\begin{figure}[!t]
  \centering
  \resizebox{2.5in}{!}{%
    \input{graphs/constraints.tex}
  }
  %ZG modified caption
  \caption{The constraint function \(\Loss_{\rm con}(x)\) for \(a=0\), \(b=1\), \(\gamma=0.0025\).}
  \label{fig:constrains}
\end{figure}

\subsection{Neural ODE based Memristor Modeling}

\noindent In this study, two neural network-based modeling approaches are proposed. The first one, denoted as \textbf{\MemODEOne}, employs a deterministic formulation of the memristor memductance \(\G\), while the function \(f\) defining the dynamics of internal variables is modeled using a neural network (see Fig.~\ref{fig:nn_structure}). In the second one, referred to as \textbf{\MemODETwo}, both \(\G\) and \(f\) are modeled using neural networks (see Fig.~\ref{fig:nn_structure_dual}).

\begin{figure}[htbp]
  \centering
  \resizebox{\columnwidth}{!}{%
    \input{tex_figures/single_MemODE.tex}
  }
  \caption{The \MemODEOne\ approach. The artificial neural network (ANN) evaluates the function \(f\!(x,v)\), which is then integrated by the ODE solver to predict the state at the next time step \(x(t + \Delta t)\). The upper dashed feedback loop indicates the recurrent nature of the process, where the output state is fed back as input for subsequent predictions. The memductance \(\Gm(x)\) is defined using a deterministic function based on the internal variable~\(x\). The function \(h(\G, v_s(t))\) computes the memristor voltage \(\vm\) given the memductance \(\G\) and supply voltage \(v_s(t)\).
  }
  \label{fig:nn_structure}
\end{figure}

\begin{figure}[htbp]
  \centering
  \resizebox{\columnwidth}{!}{%
    \input{tex_figures/dual_MemODE.tex}
  }
  \caption{The \MemODETwo\ approach. The the artificial neural network (ANN) evaluates the function \(f(\mathbf{x},v)\), which is then integrated by the ODE solver to predict the state at the next time step \(\mathbf{x}(t\!+\!\Delta t)\). The second ANN models the  memductance \(\Gm(\mathbf{x})\) based on \(\mathbf{x}(t\!+\!\Delta t)\). The function \(h(\G, v_s(t))\) computes the memristor voltage \(\vm\) given the memductance \(\G\) and supply voltage \(v_s(t)\). 
  }
  \label{fig:nn_structure_dual}
\end{figure}

For the \MemODEOne\ approach, the function \(f\) is implemented using an artificial neural network with a single output. For the \MemODETwo\ approach the number of outputs is equal to the number of internal variables and may be larger than 1. For both cases, the neural network consists of interconnected nodes (neurons) that process information through weighted connections. Each neuron in the network computes its output according to:
\begin{equation}
  y_k = \sigma\left(\sum_{i=1}^{M_k} w_{k,i} x_i + b_k\right),
\end{equation}
where \(k\) is the neuron's index, \(M_k\) is the number of inputs for the \(k\)th neuron, \(w_{k,i}\) are the weights that determine the strengths of input connections \(x_i\) of the \(k\)th neuron, \(b_k\) is the bias term that provides an adjustable threshold for neuron activation, and \(\sigma\) is the activation function (e.g., sigmoid, ReLU, or tanh) that introduces nonlinearity into the model.

In the \MemODEOne\ approach, the  memductance is defined on the basis of the physical mechanisms underlying self-directed channel (SDC) devices. In such memristors, the resistance evolves as a result of \(\mathrm{Ag^+}\) ions migration and the formation of conductive filaments. Consequently, the device memductance exhibits a continuous transition between different resistance states. This transition can be described as a weighted combination of the limiting resistance values, modulated by the internal variable \(x\)
\begin{equation}
  \Gm(x) = \frac{x}{R_{\rm ON}} + \frac{1 - x}{R_{\rm OFF}},
  \label{eq:conductance}
\end{equation}
where \(\Ron\) and \(\Roff\) denote the resistances in the low-resistance and high-resistance states, respectively. During the optimization process, the network parameters (weights \(w_{k,i}\) and biases \(b_k\)) are tuned jointly with the values of \(\Ron\) and \(\Roff\). For this approach the following constraints are applied:
\begin{itemize}
\item \(R_{\mathrm{ON}} \in (500\Ohm, R_{\mathrm{OFF}})\),
\item \(R_{\mathrm{OFF}} \in (R_{\mathrm{ON}}, 200\kOhm)\), 
\item \(x(t) \in (0, 1)\).
\end{itemize}
The lower bound for \(R_{\mathrm{ON}}\) and the upper bound for~\(R_{\mathrm{OFF}}\) are based on empirical measurements. Remaining constraints are consequences of physically meaningful values of parameters and variables. 

In the \MemODETwo approach, the  memductance is modeled using the second neural network. This neural network takes the current state \(\mathbf{x}\) as the input and produces the corresponding memductance \(\Gm(\mathbf{x})\) as the output. The memristor state \(\mathbf{x}\) is not constrained to be one-dimensional, enabling a more flexible representation of the device’s internal dynamics.
%Moreover, it is not restricted to a physically interpretable range, such as the interval \([0,1]\) in Eq.~\eqref{eq:conductance}.
The state variables are not restricted to any specific range, and hence no explicit constraints are applied to them. For this approach the only constraint is
\begin{itemize}
\item \(\Gm(\mathbf{x}) \in (5\muS, 2\mS)\).
\end{itemize}
These bounds are based on measurements. 



The learning process involves iteratively adjusting the network parameters to minimize the loss function that quantifies the difference between predicted and target outputs. This is  achieved through backpropagation algorithm combined with the gradient descent optimization, where gradients of the loss function with respect to each parameter are computed and used to update the weights and biases in the direction that reduces the overall error.



%Conceptual architectures are illustrated in Fig.~\ref{fig:nn_structure} for the \MemODEOne\ and in Fig.~\ref{fig:nn_structure_dual} for the \MemODETwo.

%ZG removed "Design"
\subsection{Neural Network Architecture}
\label{sec:nn_architecture}
\noindent The neural network training is implemented using the \texttt{JAX}, library for array-oriented numerical computation~\cite{jax2018github}, and \texttt{Equinox}~\cite{kidger2021equinox} within the \texttt{Python} programming environment, utilizing its automatic differentiation capabilities. To enable seamless integration of ordinary differential equations (ODEs) within the neural network training paradigm, the specialized \texttt{Diffrax} library is employed. This library provides differentiable ODE solvers that facilitate efficient gradient computation with respect to solution trajectories through the adjoint sensitivity method~\cite{Ricky2018, Ricky2021, kidger2021on, stumm2010new, wang2009minimal}. This approach enables end-to-end training of neural differential equation models by maintaining gradient flow through the numerical integration process, which is essential for learning dynamics of systems governed by ODEs.

%ZG replaced "embedded error estimation" by "error estimation"
Specifically, the \texttt{tsit5} (Tsitouras 5/4 Runge-Kutta)~\cite{Tsitouras2011},  adaptive Runge-Kutta solver is utilized for its superior balance between computational efficiency and numerical accuracy. This solver employs error estimation for adaptive step-size control, ensuring stable integration of the memristor dynamics while maintaining computational tractability during training.

The neural network architecture, illustrated in Fig.~\ref{fig:nn-arch}, employs an expansion–compression (diamond-shaped) topology that incorporates a double reduction in dimensionality in both the initial and final hidden layers.
This architectural configuration substantially decreases the overall number of trainable parameters and helps to mitigate vanishing and exploding gradient issues commonly encountered in deep architectures processing temporal sequences.


\begin{figure}[ht!]
  \centering
  \resizebox{2in}{!}{%
    \input{graphs/neuralODE_ver.tex}
  }
  
  \caption{Architecture of the feedforward neural network used within the Neural ODE framework for memristor dynamics modeling. The network consists of an input layer receiving two inputs, multiple hidden layers with decreasing-increasing-decreasing neuron counts \(\left(\lfloor N/2 \rfloor, N,\ldots,N, \lfloor N/2 \rfloor\right)\), activation functions between layers, and a single output. The Neural ODE block encompasses the core computational layers that approximate the function \(f(\xw(t), \ua(t))\) of the ODE system.}

  \label{fig:nn-arch}
\end{figure}

Several techniques are tested for the optimization of network weights and the \texttt{Adam} (Adaptive Moment Estimation) optimizer is selected due to its superior performance.
The choice of an optimizer and its parameters is guided by hyperparameter tuning conducted with the \texttt{Optuna} framework~\cite{Akiba2019}. The hyperparameter search space comprises the learning rate, weight decay, batch size, and architecture parameters like the width and the depth of the neural network.
The optimal configuration is selected based on minimizing the objective function on the validation set, ensuring that the model generalizes well to unseen data.

\subsection{Existing Memristor Models for Comparison}

\noindent To evaluate the effectiveness of the proposed models, their performance is compared with the performance of two existing memristor models: the Mean Metastable Switch (MMS) model~\cite{Minati2020} and the  Generalized Mean Metastable Switch Memristor (GMMS) model~\cite{Molter2016, Ostrovskii2021}.


The MMS model is a simplified representation of memristor's behavior, focusing on the average switching characteristics rather than the detailed dynamics. It captures essential features of the device by describing transitions between the high-resistance state (HRS) and the low-resistance state (LRS) based on the applied voltage. This model describes memristor's dynamics as
%ZG replaced "on" by "ON" and "off by ""OFF
%ZG merged three equations into one align environment
\begin{align}
  \label{eq:mms}
  \dert{x}       & = \frac{1}{\tau} \left( f_{\rm ON}(\ua) (1-x) - f_{\rm OFF}(\ua) x \right),              \\
  f_{\rm ON}(\ua)  & = \sigma\left(-\beta (\ua-\von)\right) \label{eq:mms5b},      \\
  f_{\rm OFF}(\ua) & = 1 - \sigma\left(\beta (\ua-\voff)\right)  \label{eq:mms5c}, 
\end{align}
where \(\sigma(z) = (1+e^{-z})^{-1}\), \(\beta= \frac{q}{kT}\), \(k\) is the Boltzmann constant, \(T\) is the absolute temperature, \(q\) represents the elementary charge, while \(\von\) and \(\voff\) denote the switching voltages to LRS and HRS states, respectively. The instantaneous memductance is defined in~\eqref{eq:conductance}.

In the GMMS model memristors are represented as a parallel connection of memory-dependent element and a Schottky diode~\cite{Molter2016, Ostrovskii2021}. This formulation extends the original metastable switch concept by incorporating nonlinear current--voltage characteristics, enabling a more accurate representation of memristor's behavior across a wide range of operating conditions.
The evolution of the state variable is governed by~\eqref{eq:mms}, while the current--voltage relationship is
\begin{align}
  \begin{split}
    \im(t)     & = \eta \imd(v, x) + (1-\eta) i_{\rm d}(v),                \\
    i_{\rm d}(v)     & = \alpha_{\rm f} e^{\beta_{\rm f} v} - \alpha_{\rm r} e^{-\beta_{\rm r} v}, \\
    \imd(v, x) & = \Gm(x) v,
  \end{split}
  \label{eq:gmms}
\end{align}
%ZG replaced "current through the device" by "current of the device"
where \(\eta\in [0,1]\) is the parameter representing the ratio of the memory dependent current \(\imd\) and total current \(\im\) flowing through the device. The parameters \(\alpha_f\), \(\beta_f\), \(\alpha_r\), and \(\beta_r\) are positive constants characterizing the forward and reverse current across the Schottky barrier.

The inclusion of additional parameters and nonlinearities in the GMMS model increases model's complexity and poses challenges for parameter optimization. An additional challenge arises due to the Schottky effect, which introduces nonlinearity into the current–voltage characteristic. When the memristor is connected in series with a resistor, this requires solving a set of differential-algebraic equations (DAEs) of the form
\begin{equation}
  \left\{
  \begin{aligned}
     & \vs=\vr+ \vm,                                                      \\
     & \dert{x}  = \frac{1}{\tau} \left( f_{\rm ON}(\vm) (1-x) - f_{\rm OFF}(\vm) x \right), 
  \end{aligned}
  \right.
  \label{eq:nonlinear_eq}
\end{equation}
where \(\vs\) denotes the supply voltage, \(\vr=\Rs i\) is the voltage across the resistor, and \(\vm\) is the voltage across the memristor.


Since the existing models considered involve multiple parameters, a hybrid optimization framework is implemented to find the parameter set that minimizes the discrepancy between model predictions and experimental data.
The optimization process consists of two stages, combining global exploration with local refinement. In the first stage, a global optimization algorithm is employed to systematically explore the parameter space and identify promising regions. Specifically, the Adaptive Differential Evolution with radius-limited sampling algorithm, available in the \verb|BlackBoxOptim| package~\cite{BlackBoxOptim} for the \texttt{Julia} programming language, is utilized to efficiently search for candidate solutions. In the second stage, local refinement is performed using the L-BFGS-B algorithm~\cite{Zhu1997}, as implemented in the \texttt{Optim.jl} library. This hybrid optimization strategy effectively balances exploration and exploitation, thereby enhancing convergence towards the optimal parameter set while mitigating the risk of premature stagnation in suboptimal regions of the parameter space.

\section{Results}
\label{sec:results}


\noindent During the hyperparameter tuning, a range of neural network architectures is systematically evaluated in order to identify the most suitable configuration for modeling memristor dynamics. The evaluation considers the number of hidden layers, the number of neurons per layer, and optimization settings (e.g. the batch size and the learning rate), with the aim of achieving a balance between model accuracy and generalization capabilities, as described in Sec.~\ref{sec:nn_architecture}. Among these settings, the batch size determines the number of training samples processed before the model parameters are updated, thereby influencing both the convergence speed and the stability of the learning process. In this study, the batch size corresponds to the number of trajectories processed in parallel before each parameter update. The learning rate, on the other hand, controls the step size of each parameter update during training. An excessively high learning rate may cause the optimization process to diverge, while a low value may result in slow convergence or being trapped in a local minimum.


In this study, the dataset consists of 18 trajectories obtained from measurements for all combinations of supply voltage amplitudes \(\Vs  \in \{0.5\V,1.0\V,1.5\V\}\) and frequencies \(f \in \{1\Hz,5\Hz,10\Hz,20\Hz,50\Hz,100\Hz\}\). The number of samples for each trajectory is \(10^5\) (100 periods of the supply voltage per trajectory, 1000 samples per period). 
This dataset is divided into the training dataset consisting of 14 trajectories and the validation datasets comprising 4 trajectories.
Trajectories used for training and validation are selected to cover a diverse range of memristor's operating conditions, thereby enhancing the model's ability to generalize across different dynamic regimes.
The validation dataset contains the following pairs of supply voltages and frequencies: \((1.5\V, 5\Hz)\), \((1\V, 1\Hz)\), \((1\V, 100\Hz)\), and \((0.5\V, 5\Hz)\).

During the hyperparameter optimization stage, the optimal configuration of the \MemODEOne\ architecture found comprises four hidden layers with 64, 128, 128, and 64 neurons in consecutive layers. The model is trained using the \texttt{AdamW} optimizer with a learning rate of approximately \(1.89 \times 10^{-2}\) and a batch size of 2. The swish activation function \(\varphi(x)=x\cdot\sigma(x)\) is employed for the hidden layers, while the final layer utilizes the linear activation function \(\varphi(x)=x\). 

For the \MemODETwo\ architecture, the optimal neural network configuration responsible for computing \(f(\xvec,\ua)\) consists of four hidden layers with 120, 240, 240, and 120 neurons in consecutive layers and with 6 latent states, corresponding to the dimensionality of the internal state vector \(\mathbf{x}\). The auxiliary neural network responsible for computing \(\G\) includes one hidden layer containing 80 neurons. Both networks are trained using the \texttt{AdaBelief} optimizer with a learning rate of approximately \(6.30 \times 10^{-3}\) and a batch size of 14. The first neural network employs the \(\mathrm{GELU}\) activation function \(\varphi(x) = x \cdot \Phi(x)   \approx 0.5 x (1 + \tanh[\sqrt{2/\pi} (x + 0.044715 x^3)]) \) in the hidden layers and the linear activation in the output layer, whereas the second network uses the \(\mathrm{ReLU}\) activation function \(\varphi(x)=\max{}(0,x)\) with the \(\mathrm{sigmoid}\) output activation.
%

A representative learning curve corresponding to the \MemODEOne\ architecture, is presented in Fig.~\ref{fig:learning-curve}.
The training process is conducted over 360 epochs, with the objective function \(\Loss\) monitored on both the training and validation datasets to provide insights into convergence behavior and potential overfitting. The learning curve demonstrates a consistent decrease in values of \(\Loss\) for both datasets, which indicates effective optimization and a satisfactory model fitting to the experimental data. The close agreement between the values of the objective functions for the training and validation datasets further confirms the generalization ability of the selected architecture.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/Training_process.pdf}
  \caption{Learning curve for the \MemODEOne\ neural network architecture during training over 360 epochs.
    The insets illustrate \(\vm\)--\(\im\) hysteresis loops at selected epochs, where the dashed lines correspond to the measured responses and the solid lines denote the model predictions.
  }
  \label{fig:learning-curve}
\end{figure}

Fig.~\ref{fig:learning-curve} also presents the \(\vm\)--\(\im\) hysteresis characteristics obtained at selected stages of the optimization process. These snapshots highlight how the model progressively refines its internal representation of the device dynamics during training. The gradual improvement of the hysteresis fit across epochs illustrates the capability of the neural network to capture the nonlinear and history-dependent behavior of the memristor, thereby validating the suitability of the chosen architecture and training configuration.


The effectiveness of optimizing the full loss function \(\Loss\) defined in~\eqref{eq:loss_function} was compared against optimizing only the base loss function~\(\LossBase\), defined in~\eqref{eq:base_loss}. For instance, when optimizing only the base loss function for the \MemODEOne\ architecture, the final value obtained for the validation dataset is \(\LossBase = 2.24 \mtimes 10^{-2}\), whereas optimizing the full loss function yields a lower value of \(\LossBase = 1.85 \mtimes 10^{-2}\). This result indicates that incorporating additional terms~\(\LossVel\) and~\(\LossCur\) improves convergence and enhances the overall predictive performance of the model, as reflected by reduced validation loss values and higher accuracy on unseen data. 

Representative simulation results demonstrating the neural network model's predictive capabilities are presented in Fig.~\ref{fig:sample-results} through comparative analysis of a tungsten-doped memristor device subjected to sinusoidal voltage excitation with an amplitude of \(1.5\;\mathrm{V}\) and frequency of \(100\;\mathrm{Hz}\).

\newcommand{\subwidth}{0.49\linewidth}
\begin{figure}[htbp]
  \centering
  \subfloat[\label{1a}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.0_freq_1.0_fig_0_test.pdf}%
  }
  % \hfill
  \subfloat[\label{1b}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.0_freq_1.0_fig_1_test.pdf}%
  }
  \\
  \subfloat[\label{1c}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.0_freq_1.0_fig_2_test.pdf}%
  }
  \caption{Simulation test results for tungsten-doped memristor dynamics under sinusoidal voltage excitation (amplitude: \(1.0\;\mathrm{V}\), frequency: \(1.0\;\mathrm{Hz}\)); (a)~memristor's voltage waveform, (b)~memristor's current response, (c)~voltage-current hysteresis loop (\(v - i\)) illustrating the characteristic pinched behavior and memory properties fundamental to memristive operation.
  }
  \label{fig:sample-results}
\end{figure}

The analysis encompasses the voltage waveform, the current waveform and the voltage-current hysteresis relationships.
The voltage profile confirms the fidelity of the input stimulus. The temporal current response reveals the device's instantaneous electrical behavior and switching kinetics. The voltage-current hysteresis loops (\(\vm - \im\)) illustrate the fundamental memory properties that define memristive behavior.


The hysteresis loops are particularly diagnostic of memristor performance, as their shape, area, and switching thresholds directly reflect the underlying ionic transport mechanisms and structural modifications responsible for resistive switching. The pinched hysteresis loop, which intersects at the origin, serves as a definitive signature of memristive behavior, while the loop area quantifies the energy dissipation associated with switching events.
These features enable comprehensive validation of the neural network model's ability to capture both the static and dynamic aspects of memristor operation.

\subsection{Comparative Analysis with the MMS and GMMS Models}

\noindent
To assess the performance of the neural ODE approach, a comprehensive comparative analysis is conducted between the developed neural network models and the existing models of SDC memristors: the Mean Metastable Switch (MMS) model and the Generalized Mean Metastable Switch (GMMS) model.


This comparison is particularly significant as the MMS and GMMS models have been extensively validated against experimental data and serve as benchmarks for memristor circuit simulation~\cite{Ostrovskii2021,Bednarz2024}. The comparative evaluation encompasses temporal voltage and current evolution, hysteretic characteristics, and quantitative loss function metrics to provide comprehensive assessment of modeling fidelity.

Comparison results are plotted in Fig.~\ref{fig:comparison_mms}. Quantitative analysis reveals substantial performance improvements achieved by the neural network approach.

\begin{figure*}[htbp]
  \centering
  \subfloat[\label{1a2}]{
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_Single-NN-MemODE.pdf}%
  }
  \subfloat[\label{1b2}]{
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_Dual-NN-MemODE.pdf}%
  }

  \subfloat[\label{1c2}]{
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_MMS Model.pdf}%
  }
  \subfloat[\label{1d2}]{
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_GMMS Model.pdf}%
  }
  \caption{Comparative analysis of the loss function \(\LossBase\):
    (a) the \MemODEOne\ model, (b) the \MemODETwo\ model, (c) the MMS model, (d) the GMMS model.
    Results for voltage amplitudes \(\Vs =0.5, 1.0, 1.5\rm \ V\) are plotted in red (\redbar{}), blue (\bluebar{}) and green (\greenbar{}), respectively.
  }
  \label{fig:comparison_mms}
\end{figure*}

Table~\ref{tab:mms_comp} presents a comparative analysis of the loss functions \(\LossBase\) obtained for the proposed and existing models, offering a quantitative assessment of their performance. The results highlight the neural network’s enhanced capability to reproduce complex and subtle aspects of memristor dynamics that are insufficiently captured by the phenomenological MMS/GMMS framework. For clarity, the highest loss value in each row is marked in red, whereas the lowest is marked in green. Overall, the neural network model consistently achieves substantially lower loss values, demonstrating superior accuracy and fidelity in representing the device behavior.



\begin{table}
  \caption{Comparative analysis of the loss function \(\LossBase\). Values are reported for different supply voltage amplitudes \(\Vs\) and frequencies \(f_s\). Row-wise minimum and maximum loss values are color-coded in green and red, respectively. Records belonging to the validation dataset are marked by bounding boxes. }
  \label{tab:mms_comp}
  \centering
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function values for \(\Vs  = 0.5 \rm \ V\)}}  \\
    \midrule
    \(f_s\) [Hz] & \makecell{SingleNN-\\MemODE} & \makecell{DualNN-\\ MemODE} & \makecell{GMMS\\Model} & \makecell{MMS\\Model}\\
    \midrule
\textbf{1} & \(9.31 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(1.22 \mtimes 10^{-3}\) & \color{ieeered} \bfseries \(3.94 \mtimes 10^{-2}\) & \(2.43 \mtimes 10^{-2}\) \\
\cline{1-5}
\multicolumn{1}{|c|}{\textbf{5}} &  \color{ieeegreen} \bfseries \(3.40 \mtimes 10^{-3}\)  &  \(6.43 \mtimes 10^{-3}\)  &  \color{ieeered} \bfseries \(2.97 \mtimes 10^{-2}\)  & \multicolumn{1}{c|}{\(1.46 \mtimes 10^{-2}\) } \\
\cline{1-5}
\textbf{10} & \(3.05 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.13 \mtimes 10^{-3}\) & \color{ieeered} \bfseries \(2.85 \mtimes 10^{-2}\) & \(2.60 \mtimes 10^{-2}\) \\
\textbf{20} & \(3.79 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(1.02 \mtimes 10^{-3}\) & \(3.25 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(1.14 \mtimes 10^{-1}\) \\
\textbf{50} & \(2.03 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(5.60 \mtimes 10^{-3}\) & \color{ieeered} \bfseries \(3.68 \mtimes 10^{-2}\) & \(3.45 \mtimes 10^{-2}\) \\
\textbf{100} & \(2.82 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(1.07 \mtimes 10^{-2}\) & \(9.91 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(1.11 \mtimes 10^{-1}\) \\
    \bottomrule
  \end{tabular}
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function values for \(\Vs  = 1 \rm \ V\)}}    \\
    \midrule
    \(f_s\) [Hz] & \makecell{SingleNN-\\MemODE} & \makecell{DualNN-\\ MemODE} & \makecell{GMMS\\Model} & \makecell{MMS\\Model}\\ 
    \midrule
\cline{1-5}
\multicolumn{1}{|c|}{\textbf{1}} &  \(5.28 \mtimes 10^{-3}\)  &  \color{ieeegreen} \bfseries \(4.47 \mtimes 10^{-3}\)  &  \(1.30 \mtimes 10^{-2}\)  & \multicolumn{1}{c|}{\color{ieeered} \bfseries \(1.63 \mtimes 10^{-2}\) } \\
\cline{1-5}
\textbf{5} & \(8.25 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.67 \mtimes 10^{-3}\) & \(1.46 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(1.71 \mtimes 10^{-2}\) \\
\textbf{10} & \(4.72 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(8.86 \mtimes 10^{-4}\) & \(1.49 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(1.58 \mtimes 10^{-2}\) \\
\textbf{20} & \(7.77 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(5.53 \mtimes 10^{-3}\) & \(1.88 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(1.90 \mtimes 10^{-2}\) \\
\textbf{50} & \(3.29 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(7.37 \mtimes 10^{-3}\) & \color{ieeered} \bfseries \(6.29 \mtimes 10^{-2}\) & \(6.22 \mtimes 10^{-2}\) \\
\cline{1-5}
\multicolumn{1}{|c|}{\textbf{100}} &  \(4.48 \mtimes 10^{-2}\)  &  \color{ieeegreen} \bfseries \(8.58 \mtimes 10^{-3}\)  &  \color{ieeered} \bfseries \(4.53 \mtimes 10^{-2}\)  & \multicolumn{1}{c|}{\(3.25 \mtimes 10^{-2}\) } \\
\cline{1-5}
    \bottomrule
  \end{tabular}
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function values for \(\Vs  = 1.5 \rm \ V\)}}  \\
    \midrule
    \(f_s\) [Hz] & \makecell{SingleNN-\\MemODE} & \makecell{DualNN-\\ MemODE} & \makecell{GMMS\\Model} & \makecell{MMS\\Model}\\
    \midrule
\textbf{1} & \(1.74 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(2.48 \mtimes 10^{-3}\) & \(1.75 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(4.74 \mtimes 10^{-2}\) \\
\cline{1-5}
\multicolumn{1}{|c|}{\textbf{5}} &  \(1.57 \mtimes 10^{-2}\)  &  \color{ieeegreen} \bfseries \(3.60 \mtimes 10^{-3}\)  &  \(2.15 \mtimes 10^{-2}\)  & \multicolumn{1}{c|}{\color{ieeered} \bfseries \(3.71 \mtimes 10^{-2}\) } \\
\cline{1-5}
\textbf{10} & \(1.55 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(3.70 \mtimes 10^{-3}\) & \(2.08 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(2.31 \mtimes 10^{-2}\) \\
\textbf{20} & \(1.32 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(7.93 \mtimes 10^{-3}\) & \(2.07 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(2.41 \mtimes 10^{-2}\) \\
\textbf{50} & \(1.37 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(8.41 \mtimes 10^{-3}\) & \(1.69 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(4.70 \mtimes 10^{-2}\) \\
\textbf{100} & \(2.64 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(1.20 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(5.54 \mtimes 10^{-2}\) & \(2.41 \mtimes 10^{-2}\) \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:mms_comp_test} reports the average loss function values \(\LossBase\) calculated for the validation, training and the whole datasets for the proposed and existing models. Results obtained for the validation dataset provide a quantitative evaluation of generalization performance of tested models across unseen memristor dynamics.
The best result of \(5.77 \times 10^{-3}\) is obtained for the \MemODETwo\ model. The significant reduction of the average values of \(\LossBase\) for both proposed method when compared to existing methods demonstrates the enhanced modeling capability achieved through the neural ODE framework.

\begin{table}
  \caption{Comparative analysis of the loss function \(\LossBase\) for the validation, training and the whole datasets. %
  }
  \label{tab:mms_comp_test}
\begin{center}
	  \begin{tabular}{c|cccc}
    \toprule
     & \makecell{SingleNN-\\MemODE} & \makecell{DualNN-\\ MemODE} & \makecell{GMMS\\Model} & \makecell{MMS\\Model}\\ 
    \midrule
     \multicolumn{5}{c}{Validation dataset} \\
    \midrule
    \(\LossBase\) & \(1.73 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(5.77 \mtimes 10^{-3}\) & \color{ieeered} \bfseries \(2.74 \mtimes 10^{-2}\) & \(2.51 \mtimes 10^{-2}\) \\
    \toprule
    \multicolumn{5}{c}{Training dataset} \\
    \midrule
    \(\LossBase\) &\(1.46 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(5.11 \mtimes 10^{-3}\) & \(3.42 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(4.22 \mtimes 10^{-2}\) \\
    \toprule
    \multicolumn{5}{c}{Whole dataset} \\
    \midrule
    \(\LossBase\) & \(1.52 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(5.26 \mtimes 10^{-3}\) & \(3.27 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(3.84 \mtimes 10^{-2}\) \\
    \bottomrule  \end{tabular}
\end{center}
\end{table}

The superior performance of the neural network model can be attributed to its capacity for learning complex nonlinear mappings directly from experimental data, whereas the MMS model relies on predetermined phenomenological relationships that may not fully capture the device-specific dynamics and material-dependent switching characteristics inherent in real memristor devices.

Let us now compare computation times needed to find different models and evaluation times of models.
Computations are carried out using a 3.5 GHz Intel i9-9900X processor.
The median training duration for the \MemODEOne\ and \MemODETwo\ architectures is approximately 11.3 minutes and 13.8 minutes, respectively. 
% when utilizing a CPU (Intel i9-9900X with a clock speed of 3.5 GHz and 10 cores)
The median computational time necessary for parameter optimization of the MMS model is approximately 20.4 minutes, whereas the GMMS model requires approximately 8.4 minutes using the same hardware. The neural network architectures are trained employing a standard deep learning framework based on gradient descent optimization algorithms. Conversely, the parameter identification methodology for the analytical models (MMS and GMMS) requires a two-stage optimization procedure. Initially, a computationally intensive global search algorithm is executed over the whole parameter space to obtain an approximate yet robust initial parameter estimate. Subsequently, a local refinement procedure employing gradient-based optimization is applied. This methodology reflects the inherent computational complexity of determining appropriate initial parameters for analytical models and accounts for their extended calibration duration relative to the single-iteration training cost of neural network-based architectures.

The computation time for trajectory generation comprising 18 trajectories using the MMS model is approximately 0.047 seconds, whereas the GMMS model requires approximately 0.051 seconds. In contrast, the \MemODETwo\ architecture needs approximately 3.97 seconds, while the \MemODEOne\ architecture requires approximately 5.56 seconds to generate an equivalent trajectory ensemble. The neural network models are integrated numerically using a relative error tolerance of \(10^{-3}\) and an absolute error tolerance of \(10^{-6}\).



\subsection{Hyperparameter Sensitivity Analysis}

\noindent Hyperparameter optimization is conducted to systematically explore the hyperparameter space and identify configurations that maximize the predictive performance of neural network architectures. The optimization employs the Tree-structured Parzen Estimator (TPE) algorithm, implemented in the \verb|Optuna| framework~\cite{akiba2019optuna}. The TPE algorithm creates a probabilistic model of the objective function by constructing two separate density estimators: one for configurations associated with above-average performance and another for those yielding below-average results. By sampling preferentially from the former, while still maintaining diversity through exploration of the latter, the algorithm achieves an efficient balance between exploitation of promising regions and exploration of the broader search space.   
The search space encompasses both architectural and training-related hyperparameters. On the architectural side, the number of hidden layers is varied between one and five, with layer widths ranging from 16 to 512 neurons in increments of 16. Candidate activation functions includes widely used nonlinearities such as \(\text{ReLU}\), \(\text{GELU}\), \(\text{SiLU}\), \(\tanh\), \(\text{ELU}\), leaky \(\text{ReLU}\), sigmoid, identity and sine functions. For the output layer, the choice is restricted to \(\text{linear}\), \(\tanh\), \(\text{sigmoid}\), and \(\text{ReLU}\). The training-related search space included the learning rate, sampled on a logarithmic scale between \(10^{-4}\) and \(10^{-1}\), batch size ranging from 1 to 18, optimizer type (Adam, SGD, AdamW, Nadam, AdaBelief), and additional parameters related to regularization and scheduling, including patience, cooldown, reduction factor, tolerance, and weight decay.

To quantify the relative influence of individual hyperparameters on the model performance, a surrogate-based importance analysis is performed. An XGBoost gradient boosting model~\cite{Chen2016} is trained on the explored configurations to predict validation loss. The surrogate model achieved an \(\mathrm{R}^2\) value of 0.99, indicating high fidelity in capturing the dependence of model performance on hyperparameter selection, where \(\mathrm{R}^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}\) represents the coefficient of determination measuring the proportion of explained variance, \(y_i\) are the observed values, \(\hat{y}_i\) are the model predictions, and \(\bar{y}\) is the mean of observed values. Importance is first estimated using the \emph{gain} metric, which reflects the cumulative improvement in predictive accuracy attributed to each hyperparameter across the ensemble. Normalization ensured that scores sum up to unity, allowing for direct interpretation as relative contributions to performance variability.

As a complementary approach, permutation importance~\cite{Altmann2010} is applied to the surrogate model. In this procedure, the values of each hyperparameter are permuted at random, and the resulting degradation in predictive accuracy is recorded. Repeated ten times for each feature, the mean error increase provides an alternative measure of importance, which, unlike gain-based metrics, also accounts for nonlinear dependencies and interactions among hyperparameters.



Results of the sensitivity analysis are presented in Fig.~\ref{fig:hyperparameter_importance}.
%\begin{itemize}
%    \item \textit{Gain-Based Scores:} 0.47, 0.27, \(2.97 \times 10^{-3}\), and 0.01, respectively.
%    \item \textit{Permutation-Based Scores:} 0.19, 0.24, 0.27, and 0.19, respectively.
%\end{itemize}
According to the XGBoost Gain approach the dominant hyperparameters are
the number of epochs,
the gradient clipping value  (the maximum allowable gradient norm),
the ANN-\(f\) depth (the depth of the neural network used to approximate \(f(\xvec,\ua)\)), and
the number of latent states, 
with the relative importance scores of 0.47, 0.27, 0.12, and 0.06, respectively. 

Collectively, the four most significant parameters account for more than 90\% of the total observed performance variance.
%(derived from summing the \changed{four} highest scores). 

The permutation importance approach identifies the following four most dominant hyperparameters influencing model performance: the learning rate, the gradient clipping value, the number of epochs, and the ANN-\(\mathcal{G}\) depth (the depth of the neural network used to approximate \(\mathcal{G}(\mathbf{x})\)).
Their relative importance scores are 0.27, 0.24, 0.19, and 0.19, respectively.

Summarizing, the most important hyperparameters are identified as the number of epochs, the gradient clipping value, the learning rate, depths of neural networks and the number of latent states. Widths of neural networks, the batch size and the step length have moderate influence. 
  The following hyperparameters have negligible effects of the model performance: the choice of activation and output functions, and the optimizer type. For example, the choice of activation function exhibits a negligible effect (importance score below \(0.01\)), indicating that the Neural ODE architecture demonstrates a high degree of robustness with respect to this factor across the examined range.
\begin{figure}[thbp]
  \centering
  \includegraphics[width=\linewidth]{mms_figs/xgboost_permutation_importance.pdf}
    \caption{Hyperparameter importance for the \MemODETwo\  optimization task. Scores are computed using
  the XG Boost surrogate model and the permutation importance approach. Larger values indicate greater relative influence of the corresponding hyperparameter on the model performance. ANN-\(\G\) and ANN-\(f\) refer to neural networks used to approximate \(\G(\xvec)\) and \(f(\xvec,\ua)\), respectively.
  }
  \label{fig:hyperparameter_importance}
\end{figure}

The mean values of the validation loss obtained during the hyperparameter optimization procedure for selected hyperparameters are presented in Fig.~\ref{fig:categorical_hyperparameters}. The figure summarizes the influence of the number of latent states, the optimizer type, and the activation functions employed in the two neural networks constituting the \MemODETwo\ architecture. For each case, the reported mean is computed by marginalizing over all remaining hyperparameters, while the corresponding \(95\%\) confidence intervals are indicated by error bars. This analysis highlights the relative sensitivity of the model performance to discrete architectural and training-strategy choices.

\begin{figure}[!t]
  \subfloat[\label{1a3}]{%
    \includegraphics[width=0.48\linewidth]{hyper_params_figs/params_latent_states_barplot.pdf}%
  }
  \subfloat[\label{1b3}]{%
    \includegraphics[width=0.48\linewidth]{hyper_params_figs/params_optimizer_barplot.pdf}%
  }

  \subfloat[\label{1c3}]{%
    \includegraphics[width=0.48\linewidth]{hyper_params_figs/params_mlpx_activation_barplot.pdf}%
  }
  \subfloat[\label{1d3}]{%
    \includegraphics[width=0.48\linewidth]{hyper_params_figs/params_mlpg_activation_barplot.pdf}%
  }
  \caption{Validation loss obtained during hyperparameter optimization for selected hyperparameters: (a) number of latent states, (b) optimizer type for the first neural network, (c) activation function for the neural network used to approximate \(f(\xvec,\ua)\), and (d) activation function for the neural network used to approximate \(\G(\xvec)\). Mean values are computed by averaging over all other hyperparameters, with error bars indicating the \(95\%\) confidence interval.}
  \label{fig:categorical_hyperparameters}
\end{figure}

\section{Conclusions}

\noindent This work presented two novel neural ODE-based architectures for modeling the dynamics of memristor devices. The proposed models leverage the power of neural networks to capture complex, nonlinear behaviors inherent in memristive systems, while maintaining the interpretability and physical consistency afforded by the ODE framework.
The proposed models were applied for modeling of real Self-Directed Channel (SDC) memristors. 
Extensive evaluation against experimental data demonstrates that the neural ODE approach significantly outperforms traditional phenomenological models, such as the MMS and GMMS models, in terms of accuracy and fidelity. The results highlight the capability of the neural ODE approach to learn intricate device dynamics directly from data, thereby providing a powerful tool for simulating and understanding memristor behavior. Hyperparameter sensitivity analysis further elucidates the critical factors influencing model performance, guiding future optimization efforts.
Obtained results show that the computation time needed to optimize the proposed models is smaller than that for existing models. On the other hand the evaluation time needed to compute trajectories using the proposed models is significantly larger than for the MMS and GMMS models.

The proposed models may also be used for modeling of memristors produced in other technologies.  
The findings of this study underscore the potential of neural ODEs as a versatile and effective framework for modeling complex physical systems, with applications in the design and analysis of memristor-based circuits.
Developed methods pave the way to more accurate and efficient simulation tools, facilitating advancements in memristor technology and its integration into emerging computing paradigms, and open avenues for future research in data-driven modeling of nonlinear dynamical systems.



\bibliographystyle{IEEEtran} % added %ZG
\bibliography{bibliography} % added %ZG

%\printbibliography %commented %ZG

% commented three lines %ZG
%\begin{IEEEbiographynophoto}{Jane Doe}
%  Biography text here without a photo.
%\end{IEEEbiographynophoto}

% commented two lines %ZG
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
%  In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}

\end{document}


