\documentclass[a4,journal]{IEEEtran} % zmiana lettersize na a4 %ZG
% ensure proper font encoding and provide Times-compatible fonts (fixes TU/ptm/b/n undefined)
\usepackage[T1]{fontenc}
\usepackage{newtxtext,newtxmath}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\ifCLASSOPTIONcompsoc
    \usepackage[caption=false, font=normalsize, labelfont=sf, textfont=sf]{subfig}
\else
\usepackage[caption=false, font=footnotesize]{subfig}
\fi
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{balance}

\usepackage[extract=python]{memoize} %ZG
% -- ADDED PACKAGES  after memoize --
\usepackage{graphicx}
\usepackage{circuitikz}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{makecell}

% optional packages - NOTES
\usepackage{ifthen}


% \usepackage{algpseudocode}


% \pgfplotsset{compat=1.18} % Set the compatibility level for pg
\usetikzlibrary{shapes.geometric, arrows, positioning, fit, backgrounds}



\definecolor{ieeeblue}{RGB}{0,118,186}      % IEEE Blue
\definecolor{ieeered}{RGB}{214,39,40}       % IEEE Red  
\definecolor{ieeegreen}{RGB}{23,156,82}     % IEEE Green
\definecolor{red}{RGB}{255,0,0}             % Red
\definecolor{blue}{RGB}{0,0,255}            % Blue
\definecolor{darkgreen}{RGB}{0,180,0}  % Dark Green


\newcounter{todocounter}

\makeatletter
\newcommand{\listoftodos}{\section*{List of TODOs}\@starttoc{tod}}
\newcommand{\l@todo}[2]{#1 \dotfill #2\par}
\makeatother

% \newcommand{\TODO}[2][]{%
%   \stepcounter{todocounter}%
%   {\color{red}#2}%
%   \ifx\relax#1\relax
%     \addcontentsline{tod}{todo}{\protect\numberline{\thetodocounter}#2}%
%   \else
%     {\color{orange}\,[#1]}%
%     \addcontentsline{tod}{todo}{\protect\numberline{\thetodocounter}#2 -- \textit{#1}}%
%   \fi
% }



\newcommand{\TODO}[2][]{%
  {\color{red} TODO: #2}%
  \ifthenelse{\equal{#1}{}}{}{%
    {\color{cyan}---#1}%
  }%
}

\newcommand\changed[2][pending]{%
  \ifthenelse{\equal{#1}{approved}}{#2}{{\color{blue}#2}}%
}


\newcommand{\der}{{\rm d}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\Ron}{R_{\rm ON}}
\newcommand{\Roff}{R_{\rm OFF}}
\newcommand{\von}{V_{\rm ON}}
\newcommand{\voff}{V_{\rm OFF}}
\newcommand{\q}{q}
\newcommand{\ua}{v}
\newcommand{\ia}{i}
\newcommand{\phia}{\varphi}
\newcommand{\xw}{x}
\newcommand{\dert}[1]{\frac{{\rm d} {#1}}{{\rm d} t} }
\newcommand{\inv}[1]{\frac{1}{#1} }
\newcommand{\equal}{=}
%ZG replaced $V_s$ by $V_{\mathrm s}$
\newcommand{\Vs}{V_{\mathrm s}}
\newcommand{\vs}{v_{\mathrm s}}
\newcommand{\vr}{v_{\mathrm r}}
\newcommand{\vm}{v_{\mathrm m}}  % memristor voltage
\newcommand{\im}{i_{\mathrm m}}  % memristor current
\newcommand{\imd}{i_{\mathrm md}} % memory dependent current
\newcommand{\Rs}{R_{\mathrm s}}

\newcommand{\redbar}{\protect\tikz[baseline=-0.1ex]\fill[ieeered] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\bluebar}{\protect\tikz[baseline=-0.1ex]\fill[ieeeblue] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\greenbar}{\protect\tikz[baseline=-0.1ex]\fill[ieeegreen] (0,0) rectangle (0.8em,1.2ex);}
\newcommand{\mtimes}{\!\times\!}

%ZG commented out
%\usepackage[ 
%  backend=biber,
%  style=ieee,
%  minnames=1,
%  maxcitenames=2, maxbibnames=6
%]{biblatex}

%\addbibresource{bibliography.bib} %ZG


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}


\begin{document}
\title{Neural ODE based modeling of memristive circuits}
\author{Karol Bednarz, Bartłomiej Garda, Zbigniew Galias,~\IEEEmembership{Senior Member,~IEEE} %ZG
  \thanks{Manuscript received Month ??, 2025. This work was supported by the National Science Centre, Poland, under Grant 2024/53/B/ST7/03841.}
  \thanks{The authors are with the Department of Electrical Engineering, AGH University of Science and Technology, al. Mickiewicza 30, 30--059, Krak\'ow, Poland, (e-mail: kbednarz@agh.edu.pl).} %ZG
}

\markboth{IEEE Transactions on Circuits and Systems,~Vol.~??, No.~?, Month~Year} %ZG
{Neural ODE based modeling of memristive circuits} %ZG

\maketitle

\begin{abstract}
\TODO{To do.} %ZG
\end{abstract}

\begin{IEEEkeywords}
  Memristor modeling, artificial neural network, neural ODE. %ZG
\end{IEEEkeywords}

% \listoftodos %KB
\section{Introduction}
\IEEEPARstart{N}{umerous} memristor models have been proposed in the scientific literature following the discovery of memristive behavior at the nanoscale. The original model, introduced in~\cite{Strukov2008}, conceptualizes the memristor as a series connection of two variable resistances corresponding to the conductive and insulating regions of the thin film. To more accurately model Self-Directed Channel (SDC) memristors, M.~Nugent and T.~Molter proposed the generalized \textit{Mean Metastable Switch} (MMS) model, which is a semi-empirical formulation. In this approach, the time derivative of the internal state variable is defined as a function of both the transition probabilities between metastable states and the current value of the state variable~\cite{Molter2016}. In~\cite{Lee2024}, the authors attempted to model memristive behavior using the \textit{Physics-Informed Neural Networks} (PINNs) framework. Although the reported results indicate the potential of this method, the study is not grounded in experimental measurements of physical memristor devices. Instead, the analysis is limited to comparisons with the outcomes of existing simulation models. Moreover, the training data employed during the neural network learning process were generated based on previously developed theoretical models, thereby limiting the ability to assess the method's accuracy in the context of real-world physical systems.


In~\cite{Ricky2018}, the authors introduce the concept of deep neural models in which the dynamics of the hidden state are governed by an \textit{ordinary differential equation} (ODE). The training process is performed in an end-to-end manner, meaning that all parameters are optimized simultaneously within a single training routine. A key innovation of the proposed approach is a novel backpropagation technique, which relies on solving the corresponding adjoint ODE backward in time using \textit{adjoint sensitivity methods}. This formulation enables efficient gradient computation and facilitates the application of neural ODEs in various architectures, including continuous-depth residual networks and generative flow-based models.


In~\cite{Ricky2021}, the authors extend the classical Neural ODE framework to enable the modeling of discontinuous events in continuous time—without requiring prior knowledge of the number or timing of such events. The proposed differentiable event functions allow for efficient simulation of hybrid systems, such as systems with collisions, state-switching mechanisms, or point processes. This development opens up new possibilities for modeling and training systems with discrete control inputs and non-smooth dynamics.


The present study aims to investigate the feasibility of modeling Self-Directed Channel (SDC) memristors using artificial neural networks, and to compare the effectiveness of this approach with that of existing theoretical models. Specifically, we focus on the application of Neural Ordinary Differential Equation (Neural ODE) models to simulate the behavior of SDC memristors, utilizing experimental data obtained from real physical systems.

Our objective is to assess whether neural network-based models can accurately reproduce the dynamic characteristics of SDC memristors, and to determine whether they offer any advantages over traditional theoretical approaches—particularly in terms of modeling flexibility, accuracy, and applicability to real-world, measurement-driven scenarios.


\section{\changed[approved]{Modeling Memristors using Neural Networks}}

%ZG removed "physical"
\noindent This study utilizes experimental data obtained \changed[approved]{from Self-Directed Channel} (SDC) memristors doped with tungsten. The structure and properties of these devices have been described in detail in the literature, e.g., in~\cite{Garda2024, Campbell2017}.

The experimental setup, illustrated in Fig.~\ref{fig:memristor_setup}, \changed[approved]{consists} of an SDC memristor connected in series with a resistor. The sinusoidal input voltage \changed[approved]{is} supplied by an arbitrary waveform generator, while the voltage signals \changed[approved]{are} measured using a data acquisition (DAQ) system.

Measurements \changed[approved]{are} carried out for various combinations of supply voltage amplitudes and frequencies. Specifically, the amplitude of the applied voltage was varied as $\Vs  \in \{0.5,\ 1.0,\ 1.5\}~\mathrm{[V]}$, and the frequency was selected from the set \(f \in \{1,\ 5,\ 10,\ 20,\ 50,\ 100\}~\mathrm{[Hz]}\).

\begin{figure}[!t]
  \centering
  \resizebox{2.5in}{!}{\input{graphs/schematic.tex}}
  \vspace{-0.5in}
  \caption{Schematic diagram of the measurement setup used for the SDC memristor. The device is connected in series with a resistor, and the input voltage is applied via an arbitrary waveform generator. Voltage measurements are acquired using a data acquisition (DAQ) system.}
  \label{fig:memristor_setup}
\end{figure}

\noindent The analyzed model \changed[approved]{of a memristor follows} the general theoretical framework of memristive devices originally introduced in~\cite{Chua1976}, and is expressed \changed[approved]{as}
\newcommand{\xvec}{\mathbf{x}}
\begin{subequations}
  \label{eq:memristor_model}
\begin{align}
  %  \begin{split{
    \ia(t)                    & = \G \left(\xvec(t),\ua(t)\right) \ua(t), \label{eq:memristor_model1}\\
    \frac{\der \xvec}{\der t} & = f\left(\xvec(t),\ua(t)\right), \label{eq:memristor_model2}
%  \end{subequation}
%  \end{split}
\end{align}
\end{subequations}
where \(\xvec\) denotes the vector of internal state variables, whose temporal evolution is governed by the \changed[approved]{function} \(f\), and \(\G\) represents the memductance of the device. \changed[approved]{The} fundamental challenge in memristor modeling lies in accurately identifying the functional forms of \(\G(\xvec,\ua)\) and \(f(\xvec,\ua)\).




\subsection{Objective Function}
\noindent The \changed[approved]{objective function $\mathcal{L}$ to be minimized in the optimization process} is based on \changed[approved]{trajectories generated by the dynamical system. It} is designed to simultaneously account for both the signal values and its dynamic structure. To this end, it is defined as the sum of four components, each serving a distinct and complementary role in the optimization process
%ZG removed the next line
%. The total loss function is given by:
%ZG replaced "\mathcal{L}_{\mathrm{total}}" by "\mathcal{L}", removed "\cdot", merged two lines into one line
\begin{align}
  \mathcal{L} & = \lambda_{\mathrm{base}} \mathcal{L}_{\mathrm{base}} + \lambda_{\mathrm{vel}} \mathcal{L}_{v} + \lambda_{\mathrm{curv}} \mathcal{L}_{c} + \sum^{N_c}_{i=1} \lambda_{\mathrm{con}_i} \mathcal{L}_{\mathrm{con}_i}(x).\label{eq:loss_function}
%  \begin{split}
%    \mathcal{L}_{\mathrm{total}} & = \lambda_{\mathrm{base}} \cdot \mathcal{L}_{\mathrm{base}} + \lambda_{\mathrm{vel}} \cdot \mathcal{L}_{v} \\
%    &+ \lambda_{\mathrm{curv}} \cdot \mathcal{L}_{c} + \sum^{N_c}_{i=1} \lambda_{\mathrm{con}_i} \cdot \mathcal{L}_{\mathrm{con}_i}(x)
%    \label{eq:loss_function}
%  \end{split}
\end{align}
\changed[approved]{The first term \(\mathcal{L}_{\mathrm{base}}\) denotes the primary error computed as} the mean squared error (MSE) between predicted and reference values. \(\mathcal{L}_{v}\) enforces accurate modeling of \changed[approved]{derivatives of state variables}, \(\mathcal{L}_{c}\) promotes consistency of the trajectories in terms of their curvature by \changed[approved]{computing MSE of second derivatives}, and \(\mathcal{L}_{\mathrm{con}}(x)\) \changed[approved]{enforces fulfillment of} physical or structural constraints that the model is required to satisfy, including
%removed "conditions of stability, nonlinearities, or"
conformity to the underlying physical model.
%ZG removed "within the overall loss function"
The weighting coefficients \(\lambda_{\mathrm{base}}\), \(\lambda_{\mathrm{vel}}\), and \(\lambda_{\mathrm{curv}}\) balance the influence of each term according to the priorities of the modeling task.
%, and each of the values were calculated during each optimization step using the Geometric Loss Strategy (GLS), described in \cite{Cipolla2018, Chen2018}. with the following algorithm presented in Algorithm~\ref{alg:adaptive_loss_balancing}.
Their values are calculated during each optimization step using the Geometric Loss Strategy \changed[approved]{(GLS)~\cite{Cipolla2018, Chen2018}} using the \changed[approved]{Algorithm~\ref{alg:adaptive_loss_balancing}}.

\begin{algorithm}[!t]
  \caption{Adaptive Loss Balancing with Clamping (Compact)}
  \label{alg:adaptive_loss_balancing}
  \begin{algorithmic}[1]
    \REQUIRE Loss terms \(\mathcal{L}_{\text{base}}, \mathcal{L}_{\text{vel}}, \mathcal{L}_{\text{curv}}\), constant \(\varepsilon > 0\), bounds \([\lambda_{\min}, \lambda_{\max}]\)
    \ENSURE Adaptive weights \(\lambda_{\text{base}}, \lambda_{\text{vel}}, \lambda_{\text{curv}}\)
    \STATE Collect losses: \(\mathbf{L} \gets [\mathcal{L}_{\text{base}}, \mathcal{L}_{\text{vel}}, \mathcal{L}_{\text{curv}}]\)
    \STATE Compute geometric mean: \\ \(\overline{\mathcal{L}}_{\text{geo}} \gets \exp\left(\frac{1}{|\mathbf{L}|} \sum_{i=1}^{|\mathbf{L}|} \ln(\mathcal{L}_i + \varepsilon)\right)\)
    \FOR{each loss \(\mathcal{L}_i \in \mathbf{L}\)}
    \STATE \(\lambda_i \gets \frac{\overline{\mathcal{L}}_{\text{geo}}}{\mathcal{L}_i + \varepsilon}\)
    \STATE \(\lambda_i \gets \text{clamp}(\lambda_i, \lambda_{\min}, \lambda_{\max})\)
    \ENDFOR
    \STATE \textbf{return} \((\lambda_{\text{base}}, \lambda_{\text{vel}}, \lambda_{\text{curv}})\)
  \end{algorithmic}
\end{algorithm}
%ZG removed "datasets", removed 'trajectory-specific"
\changed[approved]{Let us consider the sequence $(z_{k,t})_{t=1}^T$, where $z \in \{v,i,\dot{v},\dot{i},\ddot{v},\ddot{i}\}$ is a selected variable (voltage or current) or its derivative, $T$ is total number of samples and $k = 1,2,\ldots, N_{\text{traj}}$ is the sequence index.}
%For circuit trajectory datasets indexed by \(k = 1, \ldots, N_{\text{traj}}\), a trajectory-specific standardization procedure is implemented to normalize each electrical variable \(z \in \{v, i\}\) (representing voltage and current measurements, respectively). This normalization process utilizes the standard deviation computed independently for each individual trajectory sequence, ensuring statistical consistency within each temporal dataset.
%The trajectory-specific standard deviation is calculated using the unbiased sample estimator:
%The parameter \(T\) represents the temporal resolution of the trajectory, corresponding to the total number of discrete time-series sampling points within each measurement sequence.
% unbiased sample estimator:
\changed[approved]{The sequence is normalized utilizing the standard deviation to ensure statistical consistency within each dataset}.
The trajectory-specific standard deviation is calculated using the \changed[approved]{unbiased estimator}
\begin{equation}
  \sigma_{z,k} = \sqrt{\frac{1}{T-1} \sum_{t=1}^{T} \left(z_{k,t}^{\mathrm{true}} - \bar{z}_k^{\mathrm{true}}\right)^2},
\end{equation}
where the trajectory temporal mean is defined as $\bar{z}_k^{\mathrm{true}} = \frac{1}{T} \sum_{t=1}^{T} z_{k,t}^{\mathrm{true}}$.
%ZG equation moved to the text to reduce manuscript length
%\begin{equation}
%  \bar{z}_k^{\mathrm{true}} = \frac{1}{T} \sum_{t=1}^{T} z_{k,t}^{\mathrm{true}}
%\end{equation}
%
%ZG removed obvious statements to reduce manuscript length
%This trajectory-wise normalization methodology serves several critical analytical objectives in circuit analysis and machine learning applications. Primarily, it establishes scale invariance across electrical signals exhibiting disparate amplitude characteristics, thereby facilitating quantitative comparison of dynamic waveform morphologies independent of absolute magnitude variations. Furthermore,
This standardization approach enhances the numerical stability and convergence properties of optimization algorithms employed in neural network training procedures.

By implementing trajectory-specific normalization rather than global standardization, the methodology preserves the inherent temporal dynamics and statistical properties unique to each circuit configuration while simultaneously ensuring that amplitude disparities do not introduce systematic bias \changed[approved]{during predictive modeling procedures}. 
%during pattern recognition, feature extraction, or predictive modeling procedures.
This approach is particularly advantageous in scenarios involving heterogeneous circuit topologies or varying operational conditions, where maintaining the relative temporal structure is paramount for accurate system identification and dynamic analysis.
%structure of electrical phenomena is
%ZG removed to reduce length
%The neural network loss function comprises several specialized components designed to optimize memristor circuit modeling performance across multiple dynamical characteristics. Each component addresses specific aspects of the temporal and nonlinear behavior exhibited by memristive systems.

The primary component of the loss function is responsible for model fitting to experimental voltage-current data: 
%ZG removed
%, incorporating trajectory-specific normalization procedures.
%ZG removed next line
%This component minimizes the mean squared error (MSE) between predicted and ground-truth voltage and current waveforms, normalized by the standard deviation of each individual trajectory. This normalization ensures scale invariance across datasets with varying amplitude characteristics.
%
%ZG removed next line
%The base loss is formally expressed as it shown in Eq.~\eqref{eq:base_loss}:
\begin{equation}
  \begin{split}
    \mathcal{L}_{\mathrm{base}} = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}} \Bigg[ & \mathrm{MSE}\left(\frac{v_{\mathrm{pred},k}}{\sigma_{v,k}}, \frac{v_{\mathrm{true},k}}{\sigma_{v,k}}\right)          \\
                                                                                                & + \mathrm{MSE}\left(\frac{i_{\mathrm{pred},k}}{\sigma_{i,k}}, \frac{i_{\mathrm{true},k}}{\sigma_{i,k}}\right) \Bigg], %ZG added comma
  \end{split}
  \label{eq:base_loss}
\end{equation}
%ZG removed blank line
where \(v_{\mathrm{pred},k}\) and \(i_{\mathrm{pred},k}\) represent the predicted voltage and current for trajectory \(k\), while \(v_{\mathrm{true},k}\) and \(i_{\mathrm{true},k}\) denote the corresponding experimental measurements.
% moved from displayed equation to text
The mean squared error \changed[approved]{between two sequences $(\hat{y}_i)_{i=1}^n$ and $(y_i)_{i=1}^n$} of $n$ data points is defined as
$\mathrm{MSE}(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$. 
%The mean squared error for a sequence of \(n\) data points is defined as in Eq~\eqref{eq:mse}:
%\begin{equation}
%  \mathrm{MSE}(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
%  \label{eq:mse}
%\end{equation}


The first-order dynamics comparison \changed[approved]{$\mathcal{L}_{v}$} enables evaluation of temporal derivative agreement between model predictions and experimental data, particularly critical for capturing rapid memristor state transitions
%ZG removed next line to reduce length
%This component ensures that the neural network learns not only absolute signal values but also their instantaneous rates of change, which is fundamental for accurate modeling of dynamic systems exhibiting switching phenomena. The first-order loss component is defined as:
\begin{equation}
  \begin{split}
    \mathcal{L}_{v} = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}} \Bigg[ & \mathrm{MSE}\left(\frac{\tfrac{\mathrm{d}v}{\mathrm{d}t}_{\mathrm{pred},k}}{\sigma_{\dot{v},k}}, \frac{\tfrac{\mathrm{d}v}{\mathrm{d}t}_{\mathrm{true},k}}{\sigma_{\dot{v},k}}\right)          \\
                                                                                    & + \mathrm{MSE}\left(\frac{\tfrac{\mathrm{d}i}{\mathrm{d}t}_{\mathrm{pred},k}}{\sigma_{\dot{i},k}}, \frac{\tfrac{\mathrm{d}i}{\mathrm{d}t}_{\mathrm{true},k}}{\sigma_{\dot{i},k}}\right) \Bigg],%ZG added comma
  \end{split}
  \label{eq:loss_v}
\end{equation}
%ZG removed next line to reduce length
%This formulation captures the velocity characteristics of electrical variables, ensuring that transient behaviors and switching dynamics are accurately reproduced by the model.

The second-order comparison term $\mathcal{L}_{c}$ addresses trajectory curvature characteristics
%ZG removed next line to reduce length
%, which are essential for capturing subtle variations in system dynamics that manifest as nonlinear memristor behaviors. The curvature loss is formulated as:
\begin{equation}
  \begin{split}
    \mathcal{L}_{c} = \frac{1}{N_{\text{traj}}} \sum_{k=1}^{N_{\text{traj}}} \Bigg[ & \mathrm{MSE}\left(\frac{\tfrac{\mathrm{d}^2 v}{\mathrm{d}t^2}_{\mathrm{pred},k}}{\sigma_{\ddot{v},k}}, \frac{\tfrac{\mathrm{d}^2 v}{\mathrm{d}t^2}_{\mathrm{true},k}}{\sigma_{\ddot{v},k}}\right)          \\
                                                                                    & + \mathrm{MSE}\left(\frac{\tfrac{\mathrm{d}^2 i}{\mathrm{d}t^2}_{\mathrm{pred},k}}{\sigma_{\ddot{i},k}}, \frac{\tfrac{\mathrm{d}^2 i}{\mathrm{d}t^2}_{\mathrm{true},k}}{\sigma_{\ddot{i},k}}\right) \Bigg]
  \end{split}
  \label{eq:loss_c}
\end{equation}
This component facilitates superior representation of curvature properties within the phase-space trajectory, particularly important for modeling systems exhibiting complex nonlinear and highly dynamic responses. 
%ZG removed next line to reduce length
%By incorporating second-order temporal derivatives, this component ensures that acceleration characteristics and higher-order dynamical features are preserved during the learning process.

To enforce parameter bounds within specified intervals, a sophisticated constraint loss function utilizing smooth sigmoid transitions is implemented. This component prevents parameter drift beyond physically meaningful ranges while maintaining differentiability for gradient-based optimization algorithms. The constraint loss is mathematically expressed as:
%ZG \sigma used in the next equation, merged two lines into one
\begin{equation}
  \begin{split}
    \mathcal{L}_{\mathrm{con}}(x) = \mathbb{E}\left[ \left(
    {\sigma\left(-\tfrac{x-a}{f}\right)}(a-x)+{\sigma\left(\tfrac{x-b}{f}\right)}(x-b)
      \right)^{2}\right],%ZG added comma, \bigg replaced by \left or \right
  \end{split}
  \label{eq:loss_con}
\end{equation}
%ZG removed blank line
%ZG removed next line
%where the constituent parameters are defined as follows,
% \begin{itemize}
\changed[approved]{where $\sigma(z) = (1+e^{-z})^{-1}$ is} the sigmoid activation function,
%ZG removed next line
%(implicitly used),
\(a\) and \(b\) denote the lower and upper bounds of the admissible parameter range, respectively,
\(f\) is a \changed[approved]{scaling} factor controlling the steepness of the transition near the bounds, and
\(\mathbb{E}[\cdot]\) denotes the \changed[approved]{expected value} (ensemble average over samples or trajectories).
The term \changed[approved]{$\sigma(-(x-a)/f)(a-x)$} penalizes values of \(x\) below the lower bound \(a\),
while the term \changed[approved]{$\sigma((x-b)/f)(x-b)$} penalizes values of \(x\) above the upper bound \(b\).
Squaring ensures that the penalty is non-negative and grows  with the violation magnitude.

The operational characteristics of the constraint function \(\mathcal{L}_{\mathrm{con}}(x)\) \changed[approved]{are} illustrated in Figure~\ref{fig:constrains} for the parameter configuration: \(a=0\), \(b=1\), $\changed[approved]{f=0.0025}$ \TODO{(verify $f$)} over the domain \(x \in (-0.5, 1.5)\).
%ZG next line removed to save space
%This visualization demonstrates the smooth sigmoid-based transition behavior that provides continuous penalty gradients while maintaining differentiability across the parameter space. The constraint function architecture ensures that parameters within the admissible range \((0,1)\).
%ZG moved from the figure caption
%ZG removed "feasible"
The function \changed[approved]{$\mathcal{L}_{\mathrm{con}}(x)$} exhibits minimal penalty in the region \(x\in(0,1)\) and progressively increasing penalties as the \changed[approved]{parameter $x$ exceeds} the boundary limits.

\begin{figure}[!t]
  \centering
  \resizebox{2.5in}{!}{%
    \input{graphs/constraints.tex}
  }
  %ZG modified caption
  \caption{The constraint function \(\mathcal{L}_{\mathrm{con}}(x)\) for \changed[approved]{$a=0$, $b=1$, $f=0.0025$}.}
  \label{fig:constrains}
\end{figure}

%ZG the next paragraph removed to save space
%This constraint formulation provides smooth penalty gradients that guide parameters toward the feasible region while avoiding discontinuities that could disrupt the optimization process. The sigmoid-based transitions ensure that the constraint becomes progressively more restrictive as parameters approach the boundary limits, promoting stable convergence behavior.
%During neural network training, specific interval constraints were implemented to ensure physically meaningful and numerically stable parameter values that align with experimental memristor characteristics and theoretical boundaries. These constraints prevent parameter drift into unphysical regimes while maintaining the differentiability required for gradient-based optimization algorithms.

%ZG removed "critical"
\changed{The following constraint intervals are applied to
memristor model parameters for the SingleNN-MemODE approach:
%ZG removed long descriptions to save space, a shorter description is added below the list
\begin{itemize}
\item \(R_{\mathrm{ON}} \in (0, R_{\mathrm{OFF}})\,\mathrm{k}\Omega\),
\item \(R_{\mathrm{OFF}} \in (R_{\mathrm{ON}}, 200)\,\mathrm{k}\Omega\),
\item \(x(t) \in (0, 1)\),
\end{itemize}
and for the Dual-NN-MemODE approach:
\begin{itemize}
\item \(\G \in (0, 2) \rm mS\).
\end{itemize}
As the state variable \(\mathbf{x}(t)\) in the Dual-NN-MemODE approach is not restricted to a specific range, no explicit constraints are applied to it.
}
\TODO[DONE]{What is $x(t)$ for the Dual-NN-MemODE?}\\
\changed[approved]{The upper bound for the OFF-state resistance~$R_{\mathrm{OFF}}$ and bounds for the conductance parameter~$G$ are based on empirical measurements.
  Remaining constraints are consequences of physically meaningful values of parameters and variables.}\\
\TODO[DONE]{Is the objective function common for all models? If so, perhaps section ``C. Objective Function'' could be placed before section ``A. Neural ODE based Memristor Modeling''. }



\subsection{\changed[approved]{Neural ODE based Memristor Modeling}}

\TODO[DONE -- I was wondering Det - for deterministic equation, however this might be misleading]{I suggest to change the acronym for the first method to ``SingleNN-MemODE''. What is your opinion?}

\noindent In this study, two neural network-based modeling approaches are \changed[approved]{proposed}. The first \changed[approved]{one}, denoted as \textbf{SingleNN-MemODE}, employs a deterministic formulation of the memristor conductance \changed[approved]{$\G$, while the function $f$ defining the dynamics of internal variables is modeled using a neural network. In} the second \changed[approved]{one}, referred to as \textbf{Dual-NN-MemODE}, both \changed[approved]{$\G$ and $f$ are modeled} using neural networks. 


%ZG next line removed to save space
%Although both approaches utilize neural networks to approximate the \changed{function} \(f(\xvec(t), \ua(t))\), they differ fundamentally in the representation of the memristor conductance.
\changed[approved]{For the SingleNN-MemODE approach, the function $f$ is implemented using an artificial neural network with a single output. For the Dual-NN-MemODE approach the number of outputs which is equal to the number of internal variables may be larger than 1. For both cases, the} neural network consists of interconnected nodes (neurons) that process information through weighted connections. Each neuron in the network computes its output according to:
\begin{equation}
  y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right), %ZG comma added 
\end{equation}
%ZG removed empty line,
%ZG removed "and importance" 
where \(w_i\) are the weights that determine the \changed[approved]{strength of} each input connection \(x_i\), \(b\) is the bias term that provides an adjustable threshold for neuron activation, and \(\sigma\) is the activation function (e.g., sigmoid, ReLU, or tanh) that introduces non-linearity into the model.
During the optimization process, the network parameters (weights \(w_i\) and biases \(b\)) are tuned jointly with the values of \(R_{\rm ON}\) and \(R_{\rm OFF}\).
%ZG shortened
%The weights represent the learnable parameters that encode the relationship between inputs and outputs, while biases allow neurons to shift their activation threshold, enabling the network to better fit the training data even when all inputs are zero. During the optimization process, the network parameters (weights and biases) are tuned jointly with the values of \(R_{\rm ON}\) and \(R_{\rm OFF}\).

The learning process involves iteratively adjusting the network parameters to minimize \changed[approved]{the} loss function that quantifies the difference between predicted and target outputs. This is  achieved through backpropagation algorithm combined with gradient descent optimization, where gradients of the loss function with respect to each parameter are computed and used to update the weights and biases in the direction that reduces the overall error.

In the first approach (SingleNN-MemODE), the memristor conductance is defined on the basis of the physical mechanisms underlying self-directed channel (SDC) devices. In such memristors, the resistance evolves as a result of \(\mathrm{Ag^+}\) ion migration and the dynamic formation of conductive filaments. Consequently, the device conductance exhibits a continuous transition between distinct resistance states. This transition can be described as a weighted combination of the limiting resistance values, modulated by the \changed[approved]{internal variable}. % ZG: removed "state"
\changed[approved]{The relation between the internal variable and the memristor conductance has the form}
\TODO[DONE -- I think it is not needed.]{Removed definition of $M(x)$, is it needed?}
%Formally, memristor conductance is expressed as \(G_m(x) = \M(x)^{-1}\), where \(\M(x)\) denotes the state-dependent resistance.
\begin{equation}
  G_{m}(x) = \frac{x}{R_{\rm ON}} + \frac{1 - x}{R_{\rm OFF}},
  \label{eq:conductance}
\end{equation}
where \(\Ron\) and \(\Roff\) denote the resistances in the low-conductance and high-conductance states, respectively.

\changed[approved]{In the second approach (Dual-NN-MemODE), the memristor conductance is modeled using the second neural network.} \changed{This neural network takes the current state \(x(t)\) as input and produces the corresponding conductance \(G_m(x)\) as output. In this approach, the memristor state variable \(\mathbf{x}\) is not constrained to a single dimension, enabling a more flexible representation of the device’s internal dynamics. Moreover, it is not restricted to a physically interpretable range, such as the interval \([0,1]\) in Eq.~\eqref{eq:conductance}.
}

Conceptual architectures are illustrated in Fig.~\ref{fig:nn_structure} for the SingleNN-MemODE and in Fig.~\ref{fig:nn_structure_dual} for the Dual-NN-MemODE.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{figs/nn_diagram_Det_MemODE.pdf}
  %ZG replaced "computes the derivative function \(f_x(x,v)\)" by "evaluates the function \(f(x,v)\)" (subscript x removed)
  \caption{ Conceptual diagram illustrating the use of a neural network to simulate the dynamics of the memristor, for the SingleNN-MemODE, with schematic representation of the Neural ODE framework, where \changed[approved]{the} artificial neural network (ANN) evaluates \changed[approved]{the function \(f(x,v)\)}, which is then integrated by \changed[approved]{the} ODE solver to predict the state at the next time step \(x(t + \Delta t)\). The dashed feedback loop indicates the recurrent nature of the process, where the output state is fed back as input for subsequent predictions. \TODO[DONE]{Replace $f_x$ by $f$ in the figure.}}
  \label{fig:nn_structure}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{figs/nn_diagram_Dual_MemODE.pdf}
  %ZG replaced "computes the derivative function \(f_x(x,v)\)" by "evaluates the function \(f(x,v)\)" (subscript x removed)
  \caption{ Conceptual diagram illustrating the use of a neural network to simulate the dynamics of the memristor, for the Dual-NN-MemODE, with schematic representation of the Neural ODE framework, where \changed[approved]{the} artificial neural network (ANN) evaluates \changed[approved]{the function \(f(x,v)\)}, which is then integrated by \changed[approved]{the} ODE solver to predict the state at the next time step \(x(t + \Delta t)\). \changed{Additionally, a second ANN models the memristor conductance \(G_m(x)\) based on the current state \(x(t)\).}
    %ZG commented the next line
    %The dashed feedback loop indicates the recurrent nature of the process, where the output state is fed back as input for subsequent predictions.
    \TODO[DONE]{Replace $f_x$ by $f$ in the figure.}
    \TODO[DONE]{Description needed for the second ANN.}
  }
  \label{fig:nn_structure_dual}
\end{figure}

%ZG removed "Design"
\subsection{Neural Network Architecture}

%ZG replaced "model development and training pipeline was" by "training is"
%ZG removed ", described in"
%ZG replaced "leveraging" by "utilizing"
%ZG removed ", well described in"
%ZG replaced "learning dynamic systems governed by differential equations" by "learning dynamics of systems governed by ODEs"
\noindent The neural network \changed[approved]{training is} implemented using the \texttt{PyTorch} deep learning framework~\cite{paszke2019pytorchimperativestylehighperformance}, within the \texttt{Python} programming environment, \changed[approved]{utilizing} its automatic differentiation capabilities. To enable seamless integration of ordinary differential equations (ODEs) within the neural network training paradigm, the specialized \texttt{torchdiffeq} library is employed. This library provides differentiable ODE solvers that facilitate efficient gradient computation with respect to solution trajectories through the adjoint sensitivity method~\cite{Ricky2018, Ricky2021, torchdiffeq}. This approach enables end-to-end training of neural differential equation models by maintaining gradient flow through the numerical integration process, which is essential for learning \changed[approved]{dynamics of systems governed by ODEs}.

%ZG replaced "embedded error estimation" by "error estimation"
Specifically, the \texttt{dopri5} (Dormand-Prince 5th order) adaptive Runge-Kutta solver is utilized for its superior balance between computational efficiency and numerical accuracy. This solver \changed[approved]{employs error estimation} for adaptive step-size control, ensuring stable integration of the memristor dynamics while maintaining computational tractability during training.

The neural network architecture, illustrated in Figure~\ref{fig:nn-arch}, employs an expansion–compression (diamond-shaped) topology that incorporates a double reduction in dimensionality in both the initial and final hidden layers.
\changed[approved]{This architectural configuration substantially decreases the overall number of trainable parameters and helps to mitigate vanishing and exploding gradient issues commonly encountered in deep architectures processing temporal sequences.}
%ZG the list below was replaced by a short description above
%This architectural configuration effectively addresses several challenges associated with neural ODE training and the modeling of memristive systems: 
%\begin{itemize}
%  \item dimensionality reduction at the network’s input and output stages ,
%  \item the reduced parameterization facilitates more stable optimization dynamics by limiting the complexity of the parameter search space,
%  \item the controlled dimensional transitions help mitigate vanishing and exploding gradient issues commonly encountered in deep architectures processing extended temporal sequences,
%  \item the compression phase at the output acts as an implicit regularizer, constraining the learned representations to a compact, low-dimensional subspace consistent with the underlying physical behavior of memristor dynamics.
%\end{itemize}
%KB: I think the list is not needed, the description above is enough.

\begin{figure}[ht!]
  \centering
  \resizebox{2in}{!}{%
    \input{graphs/neuralODE_ver.tex}
  }
  %ZG replaced (\(\lfloor N/2 \rfloor, N, \lfloor N/2 \rfloor\)) $\(\left(\lfloor N/2 \rfloor, N, \lfloor N/2 \rfloor\right)\)$ by
  %ZG removed "derivative", replaced "in the differential equation system" by "of the ODE system". 
  \caption{Architecture of the feedforward neural network used within the Neural ODE framework for memristor dynamics modeling. The network consists of an input layer receiving two inputs, multiple hidden layers with decreasing-increasing-decreasing neuron counts \changed[approved]{$\left(\lfloor N/2 \rfloor, N,\ldots,N, \lfloor N/2 \rfloor\right)$}, activation functions between layers, and a single output. The Neural ODE block encompasses the core computational layers that approximate the function \(f(\xw(t), \ua(t))\) \changed[approved]{of the ODE system}.}

  \label{fig:nn-arch}
\end{figure}

%ZG replaced "The network weights were optimized using several optimization techniques, including the \texttt{Adam} (Adaptive Moment Estimation) optimizer." by "Several techniques were tested for the optimization of network weights and the \texttt{Adam} (Adaptive Moment Estimation) optimizer was selected due to its superior performance."
\changed[approved]{Several techniques are tested for the optimization of network weights and the \texttt{Adam} (Adaptive Moment Estimation) optimizer is selected due to its superior performance.}
%ZG removed ", as described in"
The choice of an optimizer and its parameters is guided by hyperparameter tuning conducted with the \texttt{Optuna} framework~\cite{Akiba2019}. The hyperparameter search space comprises the learning rate, weight decay, and the batch size, or architecture parameters like the width or the depth of the neural network.
%removed ", while"
The optimal configuration was selected based on test loss minimization.  \changed{This approach focuses on minimizing the loss on the test set, ensuring that the model generalizes well to unseen data.} \TODO[DONE]{What is ``validation loss minimization''?}

% To enable adaptive control of the learning rate throughout training, the \texttt{ReduceLROnPlateau} scheduling strategy was employed. This performance-based scheduler dynamically adjusts the learning rate in response to plateaus in the monitored validation loss, thereby improving convergence stability. Specifically, the scheduler reduces the learning rate once the monitored loss stagnates, applies a predefined reduction factor, incorporates a patience interval before making adjustments, and enforces lower and upper bounds to prevent instability. This implementation strategy ensures robust and efficient training of neural differential equation models for memristor circuit simulation, integrating state-of-the-art deep learning practices with domain-specific techniques for dynamic system modeling.

%ZG next paragraph removed to save space
%These constraints are particularly crucial for memristor modeling because they preserve the fundamental physical relationships governing resistive switching phenomena while preventing the emergence of artifacts that could arise from parameter values outside the feasible operating regime.

\subsection{Existing Memristor Models for Comparison}

%ZG removed "introduced in", removed " described in"
\noindent To evaluate the effectiveness of the proposed models, their performance is compared with the performance of two existing memristor models: the Mean Metastable Switch (MMS) model~\cite{Minati2020} and the  Generalized Mean Metastable Switch Memristor (GMMS) model~\cite{Molter2016, Ostrovskii2021}.

%\subsubsection{Mean Metastable Switch (MMS) Model}

\changed[approved]{The MMS model} is a simplified representation of memristor's behavior, focusing on the average switching characteristics rather than the detailed dynamics. It captures essential features by modeling the device as a two-state system, where the resistance can switch between a high-resistance state (HRS) and a low-resistance state (LRS) based on the applied voltage. This model describes the memristor dynamics as
%ZG replaced "on" by "ON" and "off by ""OFF
%ZG merged three equations into one align environment
\begin{align}
  \label{eq:mms}
  \dert{x} & = \frac{1}{\tau} \left( f_{\rm ON}(t) (1-x) - f_{\rm OFF}(t) x \right), \\%ZG added comma, replaced "\big[" by "\left(" and "\big]" by $\right)$
%\end{align}
%where
%\begin{align}
  f_{\rm ON}(t)  & = \changed[approved]{\sigma\left(-\beta (\ua(t)-\von)\right)} \label{eq:mms5b}, \\%ZG added comma, replaced "\frac{1}{1+e^{-\beta (\ua(t)-\von)}}" by \sigma(-\beta (\ua(t)-\von))
  f_{\rm OFF}(t) & = 1 - \changed[approved]{\sigma\left(\beta (\ua(t)-\voff)\right)}  \label{eq:mms5c}, %ZG added comma, replaced "\frac{1}{1+e^{-\beta (\ua(t)-\voff)}}" by
\end{align}
%ZG removed empty line
%ZG replaced "\(\von\) denotes the switching voltage to the low-resistance state (LRS), \(\voff\) denotes the switching voltage to the high-resistance state (HRS)" by "$\von$ and $\voff$ denote the switching voltages to the low-resistance state (LRS) and the high-resistance state (HRS)
%replaced "as in Eq.~\eqref{eq:conductance}" by "in~\eqref{eq:conductance}"
where \changed[approved]{$\sigma(z) = (1+e^{-z})^{-1}$,} \(\beta= \frac{q}{kT}\), \(k\) is the Boltzmann constant, \(T\) is the absolute temperature, \(q\) represents the elementary charge, \changed[approved]{while $\von$ and $\voff$ denote the switching voltages to LRS and HRS states, respectively}. The instantaneous conductance of the memristor is defined \changed[approved]{in~\eqref{eq:conductance}}.

%\subsubsection{Generalized Mean Metastable Switch (GMMS) Model}
%ZG replaced "In~\autocite{Molter2016, Ostrovskii2021}" by "\cite{Molter2016, Ostrovskii2021}"
%ZG removed "the authors introduced the Generalized Mean Metastable Switch (GMMS) model, in which the memristor is represented as a parallel connection of two memristors and a Schottky diode"
\changed[approved]{In the GMMS model the memristor is represented as a parallel connection of \changed[approved]{memory-dependent element} and a Schottky diode~\cite{Molter2016, Ostrovskii2021}}. This formulation extends the original metastable switch concept by incorporating nonlinear current--voltage characteristics, enabling a more accurate representation of memristor's behavior across a wide range of operating conditions.
The evolution of the state variable is governed by~\eqref{eq:mms}, while the current--voltage relationship is
\begin{align}
  \begin{split}
    \im(t)      & = \phi \imd(v, x) + (1-\phi) i_d(v), \\ %ZG added comma
    i_d(v)    & = \alpha_f e^{\beta_f v} - \alpha_r e^{-\beta_r v}, \\ %ZG added comma
    \imd(v, x) & = \G_m(x) v,                           %ZG added comma
  \end{split}
  \label{eq:gmms}
\end{align}
%ZG replaced "current through the device" by "current of the device"
where \(\phi \in [0,1]\) is a parameter representing the \changed[approved]{ratio of the memory dependent current $\imd$ and total current $\im$ flowing through the device}. The parameters \(\alpha_f\), \(\beta_f\), \(\alpha_r\), and \(\beta_r\) are positive constants characterizing the forward and reverse current behavior along the Schottky barrier.
\TODO[I think this is appropriate, I've using original notation from paper before]{Change of notations. $\imd$ - memory dependent current, previously $\im$, $\im$ - the total current, previously $i$. Please confirm whether this is ok.}

%ZG removed Eq.~\eqref{eq:nonlinear_eq}
The inclusion of additional parameters and nonlinearities \changed[approved]{in the GMMS model} increases model's complexity and poses challenges for parameter optimization.An additional challenge arises due to the Schottky effect, which introduces nonlinearity into the current–voltage characteristic. When the memristor is connected in series with a resistor, this requires solving \changed[approved]{a set of differential-algebraic equations (DAEs)} of the form 
\begin{equation}
  \left\{
  \begin{aligned}
     & \vs(t)    = \vr(t) + \vm(t),\\%ZG added comma                              
     & \dert{x}  = \frac{1}{\tau} \left( f_{\rm ON}(t) (1-x) - f_{\rm OFF}(t) x \right), %ZG added comma, replaced "\big[" by "\left(" and "\big]" by $\right)$
  \end{aligned}
  \right.
  \label{eq:nonlinear_eq}
\end{equation}
where \(\vs(t)\) denotes the supply voltage, \(\vr(t) = \Rs i(t)\) is the voltage across the resistor, and \(\vm(t)\) is the voltage across the memristor.

%\subsubsection{Parameter Optimization}
%ZG removed "determinitic, removed "as" in "as implemented"

Since the existing models considered involve multiple parameters, a hybrid optimization framework is implemented to find the parameter set that minimizes the discrepancy between model predictions and experimental data.
%ZG removed next sentence
%In comparison to neural network--based models, the number of parameters to be optimized is significantly smaller, which facilitates the calibration process.
The optimization process consists of two stages, combining global exploration with local refinement. In the first stage, a global optimization algorithm is employed to systematically explore the parameter space and identify promising regions. Specifically, the Adaptive Differential Evolution with radius-limited sampling algorithm, available in the \verb|BlackBoxOptim| package~\cite{BlackBoxOptim} for the \texttt{Julia} programming language, was utilized to efficiently search for candidate solutions. In the second stage, local refinement was performed using the L-BFGS-B algorithm~\cite{Zhu1997}, as implemented in the \texttt{Optim.jl} library. This hybrid optimization strategy effectively balances exploration and exploitation, thereby enhancing convergence towards the optimal parameter set while mitigating the risk of premature stagnation in suboptimal regions of the parameter space.

\section{Results}
\noindent During the preliminary experiments, a range of neural network architectures was systematically evaluated in order to identify the most suitable configuration for modeling memristor dynamics. The evaluation considered different numbers of hidden layers, neurons per layer, and optimization settings, with the aim of achieving a balance between model accuracy, generalization capability, and computational efficiency.

\TODO{The notions of the batch size and learning rates should be explained.}\\
\TODO{Sizes of training and validation datasets should be given.}\\
A representative learning curve corresponding to the SingleNN-MemODE architecture, consisting of two hidden layers with 128 neurons each, trained using the \verb|Adam| optimizer with a learning rate of \(10^{-2}\) and a batch size of 2, is presented in Figure~\ref{fig:learning-curve}.
The training process was conducted over 360 epochs, with the objective function $\mathcal{L}$ monitored on both the training and validation datasets to provide insights into convergence behavior and potential overfitting. The learning curve demonstrates a consistent decrease
%ZG removed "and smooth"
in values of $\mathcal{L}$ for both datasets, which indicates effective optimization and a satisfactory model fitting to the experimental data. The close agreement between the training and validation loss further confirms the generalization ability of the selected architecture.

In addition to the learning curve, Figure~\ref{fig:learning-curve} also presents the \(\vm\)--\(\im\) hysteresis characteristics obtained at selected stages of the optimization process. These snapshots highlight how the model progressively refines its internal representation of the device dynamics during training. The gradual improvement of the hysteresis fit across epochs illustrates the capability of the neural network to capture the nonlinear and history-dependent behavior of the memristor, thereby validating the suitability of the chosen architecture and training configuration.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figs/Training_process.pdf}
  \caption{Learning curve for the SingleNN-MemODE neural network architecture during training over 360 epochs.
    %The figure depicts a gradual decrease in the loss function values for both the training and validation datasets, reflecting effective optimization and adequate convergence of the model toward the experimental memristor data. %ZG removed
    The insets illustrate the evolution of the \( \vm - \im \) hysteresis loops at selected epochs, where the dashed lines correspond to the measured responses and the solid lines denote the model predictions.
    %These results demonstrate the model’s ability to accurately reproduce the nonlinear and history-dependent characteristics of the memristor as training advances. %ZG removed
  }
  \label{fig:learning-curve}
\end{figure}

Representative simulation results demonstrating the neural network model's predictive capabilities are presented in Figure~\ref{fig:sample-results} through comparative analysis of a tungsten-doped memristor device subjected to sinusoidal voltage excitation with an amplitude of $1.5\;\mathrm{V}$ and frequency of $100\;\mathrm{Hz}$.
%ZG removed the line below to save space
%These specific excitation parameters were selected to capture the characteristic switching dynamics while maintaining operation within the device's linear regime, enabling comprehensive evaluation of the model's ability to reproduce fundamental memristive behaviors.
The analysis \changed[approved]{encompasses the voltage waveform, the current waveform and the voltage-current hysteresis relationships}.
\changed[approved]{The voltage profile confirms the fidelity of the input stimulus. The temporal current response reveals the device's instantaneous electrical behavior and switching kinetics. The voltage-current hysteresis loops (\(\vm - \im\)) illustrate the fundamental memory properties that define memristive behavior.}
%ZG above is a shortened version of the paragraph below
%The analysis encompasses three critical dynamic characteristics that collectively characterize memristor behavior: temporal current evolution, applied voltage profiles, and voltage-current hysteresis relationships.
%The temporal current response reveals the device's instantaneous electrical behavior and switching kinetics, while the applied voltage profile confirms the fidelity of the input stimulus.

The hysteresis loops are particularly diagnostic of memristor performance, as their shape, area, and switching thresholds directly reflect the underlying ionic transport mechanisms and structural modifications responsible for resistive switching. The pinched hysteresis characteristic observed at the origin serves as a definitive signature of memristive behavior, while the loop area quantifies the energy dissipation associated with switching events. These features enable comprehensive validation of the neural network model's ability to capture both the static and dynamic aspects of memristor operation.

\newcommand{\subwidth}{0.49\linewidth}
\begin{figure}[!t]
  \centering
  \subfloat[\label{1a}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.5_freq_100.0_fig_0.pdf}%
  }
  % \hfill
  \subfloat[\label{1b}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.5_freq_100.0_fig_1.pdf}%
  }
  \\
  \subfloat[\label{1c}]{%
    \includegraphics[width=\subwidth]{sample_results/sol_plots_amp_1.5_freq_100.0_fig_2.pdf}%
  }
  %ZG removed "Representative"
  \caption{Simulation results for tungsten-doped memristor dynamics under sinusoidal voltage excitation (amplitude: $1.5\;\mathrm{V}$, frequency: $100\;\mathrm{Hz}$); (a) memristor's voltage waveform, (b) memristor's current response, (c) Voltage-current hysteresis loop (\(v_{\mathrm{m}} - i_{\mathrm{m}}\)) illustrating the characteristic pinched behavior and memory properties fundamental to memristive operation.
    %The neural network model successfully captures the essential features of memristor dynamics including switching thresholds, nonlinear conductance modulation, and hysteretic memory effects. %ZG removed
  }
  \label{fig:sample-results}
\end{figure}


\subsection{Comparative Analysis with \changed[approved]{the MMS and GMMS Models}}

\noindent
To assess the performance of the neural ODE approach, a comprehensive comparative analysis is conducted between the developed neural network models and the existing models of SDC memristors: the Mean Metastable Switch (MMS) model and the Generalized Mean Metastable Switch (GMMS) model.

%ZG \autocite -> \cite
%ZG removed "across multiple performance dimensions"
This comparison is particularly significant as the MMS and GMMS models have been extensively validated against experimental data and serve as a benchmark for memristor circuit simulation in the scientific literature~\cite{Bednarz2024, Ostrovskii2021}. The comparative evaluation encompasses temporal voltage and current evolution, hysteretic characteristics, and quantitative loss function metrics to provide comprehensive assessment of modeling fidelity. 

\changed[approved]{Comparison results are plotted in Fig.~\ref{fig:comparison_mms}.} Quantitative analysis reveals substantial performance improvements achieved by the neural network approach.

\begin{figure*}[!t]
  \centering
  \subfloat[\label{1a2}]{% "1a" -> "1a2" %ZG to avoid multiple labels
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_Det-MemODE.pdf}%
  }
  \subfloat[\label{1b2}]{% "1b" -> "1b2" %ZG
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_Dual-NN-MemODE.pdf}%
  }
  
  \subfloat[\label{1c2}]{% "1c" -> "1c2" %ZG
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_MMS Model.pdf}%
  }
  \subfloat[\label{1d2}]{% "1d" -> "1d2" %ZG
    \includegraphics[width=0.48\linewidth]{mms_results/loss_per_freq_GMMS Model.pdf}%
  }
  \caption{Comparative analysis of loss function values:
    (a) the SingleNN-MemODE model, (b) the Dual-NN-MemODE model, (c) the MMS model, (d) the GMMS model. 
    \changed[approved]{Results for voltage amplitudes \(\Vs =0.5, 1.0, 1.5\rm \ V\) are plotted in red (\redbar{}), blue (\bluebar{}) and green (\greenbar{}), respectively}. 
%      where the following colors distinguish different supply : (\redbar{})~\(\Vs =0.5\rm \ V\), (\bluebar{})~\(\Vs =1.0\rm \ V\), and (\greenbar{}) \(\Vs =1.5\rm \ V\).
    %The neural network demonstrates superior convergence characteristics and achieves significantly lower final loss values, indicating enhanced modeling accuracy and better agreement with experimental memristor dynamics. %ZG removed
  }
  \label{fig:comparison_mms}
\end{figure*}

%ZG removed "representative", removed "across key electrical characteristics"
Table~\ref{tab:mms_comp} presents a comparative analysis of the loss functions $\mathcal{L}_{\mathrm{base}}$ obtained for the proposed and existing models, offering a quantitative assessment of their performance. The results highlight the neural network’s enhanced capability to reproduce complex and subtle aspects of memristor dynamics that are insufficiently captured by the phenomenological MMS/GMMS framework. For clarity, the highest loss value in each row is marked in red, whereas the lowest is marked in green. Overall, the neural network model consistently achieves substantially lower loss values, demonstrating superior accuracy and fidelity in representing the device behavior.

\begin{table}
  \caption{Comparative \(\mathcal{L}_{\mathrm{base}}\) loss function analysis.}
  \label{tab:mms_comp}
  \centering
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function comparison for \(\Vs  = 0.5 \rm \ V\)}}                                                                                            \\
    \midrule
    \(f_s\) [Hz] & \makecell{Det-                                                                                                                                              \\MemODE}             & \makecell{Dual-NN-                                                                                                             \\ MemODE}                 & \makecell{GMMS\\Model}             & \makecell{MMS\\Model}                                        \\
    \midrule
    \textbf{1}   & \(1.92 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.07 \mtimes 10^{-4}\) & \(5.21 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(7.86 \mtimes 10^{-2}\) \\
    \textbf{5}   & \(1.60 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.85 \mtimes 10^{-4}\) & \(3.70 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(5.59 \mtimes 10^{-2}\) \\
    \textbf{10}  & \(3.64 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(1.83 \mtimes 10^{-3}\) & \(3.43 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(6.44 \mtimes 10^{-2}\) \\
    \textbf{20}  & \(8.69 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.27 \mtimes 10^{-3}\) & \(4.24 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(7.96 \mtimes 10^{-2}\) \\
    \textbf{50}  & \(7.86 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.47 \mtimes 10^{-3}\) & \(4.71 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(8.29 \mtimes 10^{-2}\) \\
    \textbf{100} & \(6.24 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(4.21 \mtimes 10^{-3}\) & \(1.18 \mtimes 10^{-1}\) & \color{ieeered} \bfseries \(2.10 \mtimes 10^{-1}\) \\
    \bottomrule
  \end{tabular}
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function comparison for \(\Vs  = 1 \rm \ V\)}}                                                                                              \\
    \midrule
    \(f_s\) [Hz] & \makecell{Det-                                                                                                                                              \\MemODE}             & \makecell{Dual-NN-                                                                                                             \\ MemODE}                 & \makecell{GMMS\\Model}             & \makecell{MMS\\Model}                                        \\
    \midrule
    \textbf{1}   & \(6.40 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(4.69 \mtimes 10^{-4}\) & \(1.20 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(3.08 \mtimes 10^{-2}\) \\
    \textbf{5}   & \(4.58 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(2.11 \mtimes 10^{-3}\) & \(1.19 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(3.37 \mtimes 10^{-2}\) \\
    \textbf{10}  & \(2.74 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(5.27 \mtimes 10^{-4}\) & \(1.36 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(3.93 \mtimes 10^{-2}\) \\
    \textbf{20}  & \(5.32 \mtimes 10^{-3}\) & \color{ieeegreen} \bfseries \(1.90 \mtimes 10^{-3}\) & \(1.84 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(4.43 \mtimes 10^{-2}\) \\
    \textbf{50}  & \(2.88 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(4.26 \mtimes 10^{-3}\) & \(5.86 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(8.91 \mtimes 10^{-2}\) \\
    \textbf{100} & \(5.08 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(2.63 \mtimes 10^{-3}\) & \(5.45 \mtimes 10^{-2}\) & \color{ieeered} \bfseries \(7.45 \mtimes 10^{-2}\) \\
    \bottomrule
  \end{tabular}
  \begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Loss function comparison for \(\Vs  = 1.5 \rm \ V\)}}                                                                                                                      \\
    \midrule
    \(f_s\) [Hz] & \makecell{Det-                                                                                                                                                                        \\MemODE}             & \makecell{Dual-NN-                                                                                                             \\ MemODE}                 & \makecell{GMMS\\Model}             & \makecell{MMS\\Model}                                        \\
    \midrule
    \textbf{1}   & \(1.99 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(2.95 \mtimes 10^{-3}\) & \(1.77 \mtimes 10^{-2}\)                           & \color{ieeered} \bfseries \(9.95 \mtimes 10^{-2}\) \\
    \textbf{5}   & \(1.17 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(1.26 \mtimes 10^{-3}\) & \(1.73 \mtimes 10^{-2}\)                           & \color{ieeered} \bfseries \(7.88 \mtimes 10^{-2}\) \\
    \textbf{10}  & \(1.14 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(2.08 \mtimes 10^{-3}\) & \(1.56 \mtimes 10^{-2}\)                           & \color{ieeered} \bfseries \(5.24 \mtimes 10^{-2}\) \\
    \textbf{20}  & \(1.09 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(5.51 \mtimes 10^{-3}\) & \(1.96 \mtimes 10^{-2}\)                           & \color{ieeered} \bfseries \(5.09 \mtimes 10^{-2}\) \\
    \textbf{50}  & \(1.09 \mtimes 10^{-2}\) & \color{ieeegreen} \bfseries \(3.46 \mtimes 10^{-3}\) & \(1.87 \mtimes 10^{-2}\)                           & \color{ieeered} \bfseries \(8.90 \mtimes 10^{-2}\) \\
    \textbf{100} & \(2.44 \mtimes 10^{-2}\) & \color{ieeegreen} \(1.65 \mtimes 10^{-3}\)           & \color{ieeered} \bfseries \(5.63 \mtimes 10^{-2}\) & \(4.87 \mtimes 10^{-2}\)                           \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:mms_comp_test} reports the average loss function values $\mathcal{L}_{\mathrm{base}}$ calculated on the test dataset for the proposed and existing models, providing a quantitative evaluation of their generalization performance across unseen memristor dynamics.
The best result of \(8.82 \times 10^{-3}\) is obtained for the Dual-NN-MemODE model. The above two times reduction of the average values of the loss function for both proposed method when compared to existing methods demonstrates the enhanced modeling capability achieved through the neural ODE framework. 
%The average test loss function $\mathcal{L}_{\mathrm{base}}$ values \changed{are} \(1.82 \times 10^{-2}\) and \(8.82 \times 10^{-3}\) \changed{for the Det-MemODE and Dual-NN-MemODE models, respectively. This represents} a significant improvement compared to the \changed{average loss function values of \(5.30 \times 10^{-2}\) and \(4.19 \times 10^{-2}\) for the MMS and GMMS models}.
%ZG line below removed to save space
%This near two times reduction in loss indicates markedly superior agreement between neural network predictions and experimental data, demonstrating the enhanced modeling capability achieved through the neural differential equation framework.

%ZG removed the paragraph below to save space
%The results underscore \changed{the superior ability of neural network approach} to accurately predict complex device behavior beyond the training data, as evidenced by consistently lower loss values compared to the MMS/GMMS models. This indicates that the neural differential equation framework not only captures intricate nonlinear relationships during training but also effectively generalizes to novel scenarios, highlighting its robustness and adaptability in modeling memristive systems.

\begin{table}
  \caption{Comparative \(\mathcal{L}_{\mathrm{base}}\) loss function analysis for test dataset.}
  \label{tab:mms_comp_test}
  \begin{tabular}{c|cccc}
    \toprule

                                  & \makecell{Det-                                                                                                                                  \\MemODE}             & \makecell{Dual-NN-                                                                                                             \\ MemODE}                 & \makecell{GMMS\\Model}             & \makecell{MMS\\Model}                                        \\
    \midrule
    $\mathcal{L}_{\mathrm{base}}$ & $1.82 \mtimes 10^{-2}$ & \color{ieeegreen} \bfseries $8.28 \mtimes 10^{-3}$ & $4.19 \mtimes 10^{-2}$ & \color{ieeered} \bfseries $5.30 \mtimes 10^{-2}$ \\
    % $\mathcal{L}_{\mathrm{v}}$    & $2.96 \mtimes 10^{-1}$                          & \color{ieeegreen} \bfseries $2.22 \mtimes 10^{-1}$ & \color{ieeered} \bfseries $1.36 $   & $7.28 \mtimes 10^{-1}$                          \\
    % $\mathcal{L}_{\mathrm{c}}$    & \color{ieeered} \bfseries $3.67 \mtimes 10^{2}$ & $2.39 \mtimes 10^{2}$                              & \color{ieeegreen} \bfseries $2.10 $ & $5.02 $                                        \\
    \bottomrule
  \end{tabular}
\end{table}

The superior performance of the neural network model can be attributed to its capacity for learning complex nonlinear mappings directly from experimental data, whereas the MMS model relies on predetermined phenomenological relationships that may not fully capture the device-specific dynamics and material-dependent switching characteristics inherent in experimental memristor devices.

\subsection{Hyperparameter Sensitivity Analysis}
\TODO{It might be of interest to potential readers :-).}

\noindent Hyperparameter optimization was conducted to systematically explore the parameter space and identify configurations that maximize the predictive performance of the neural network. The optimization employed the Tree-structured Parzen Estimator (TPE) algorithm, implemented in the \verb|Optuna| framework~\cite{akiba2019optuna}. The TPE algorithm models the objective function probabilistically by constructing two separate density estimators: one for configurations associated with above-average performance and another for those yielding below-average results. By sampling preferentially from the former, while still maintaining diversity through exploration of the latter, the algorithm achieves an efficient balance between exploitation of promising regions and exploration of the broader search space.

The search space encompassed both architectural and training-related hyperparameters. On the architectural side, the number of hidden layers was varied between one and five, with layer widths ranging from 16 to 512 neurons in increments of 16. Candidate activation functions included widely used nonlinearities such as ReLU, GELU, SiLU, \(\tanh\), ELU, leaky ReLU, and sigmoid, as well as identity and sine functions. For the output layer, the choice was restricted to \({\text{linear}, \tanh, \text{sigmoid}, \text{ReLU}}\). The training-related search space included the learning rate, sampled on a logarithmic scale between \(10^{-4}\) and \(10^{-1}\), batch size ranging from 1 to 18, optimizer type (Adam, SGD, AdamW, Nadam, AdaBelief), and additional parameters related to regularization and scheduling, including patience, cooldown, reduction factor, tolerance, and weight decay.

To quantify the relative influence of individual hyperparameters on model performance, a surrogate-based importance analysis was performed. An XGBoost gradient boosting model~\cite{Chen2016} was trained on the explored configurations to predict validation loss. The surrogate achieved an \(\mathrm{R}^2\) value of 0.99, indicating high fidelity in capturing the dependence of model performance on hyperparameter selection. Importance was first estimated using the \emph{gain} metric, which reflects the cumulative improvement in predictive accuracy attributable to each hyperparameter across the ensemble. Normalization ensured that scores summed to unity, allowing for direct interpretation as relative contributions to performance variability.

As a complementary approach, permutation importance~\cite{Altmann2010} was applied to the surrogate model. In this procedure, the values of each hyperparameter were permuted at random, and the resulting degradation in predictive accuracy was recorded. Repeated ten times for each feature, the mean error increase provided an alternative measure of importance, which, unlike gain-based metrics, also accounts for nonlinear dependencies and interactions among hyperparameters.

Both approaches consistently identified the learning rate and gradient clipping value as the dominant hyperparameters. According to the gain-based measure, their contributions were 0.10 and 0.74, respectively, while the permutation-based analysis yielded corresponding values of 0.44 and 0.30. Together, these two parameters explained approximately 63\% of the observed variance in performance. By contrast, the activation function exhibited only marginal influence (importance score of 0.04), indicating that the Neural ODE architecture is relatively robust to this choice within the tested range.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{mms_figs/xgboost_permutation_importance.pdf}
  \caption{Hyperparameter importance for the Dual-NN-MemODE optimization task. Scores were computed using permutation importance applied to an XGBoost surrogate model trained on 200 hyperparameter configurations. Larger values indicate greater relative influence of the corresponding hyperparameter on model performance.}
  \label{fig:hyperparameter_importance}
\end{figure}

\TODO[That would be interesting to explore.]{What is the importance of optimizer type (mentioned in the search space). It would be interesting to know what are the obtained values of the loss function when using different values of certain hyperparameters (other parameters should be ones used for computations).}

\bibliographystyle{IEEEtran} % added %ZG
\bibliography{bibliography} % added %ZG

%\printbibliography %commented %ZG

% commented three lines %ZG
%\begin{IEEEbiographynophoto}{Jane Doe}
%  Biography text here without a photo.
%\end{IEEEbiographynophoto}

% commented two lines %ZG
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1.png}}]{IEEE Publications Technology Team}
%  In this paragraph you can place your educational, professional background and research and other interests.\end{IEEEbiography}

\end{document}


